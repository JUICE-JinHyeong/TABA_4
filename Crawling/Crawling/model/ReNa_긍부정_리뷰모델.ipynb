{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JfWy7bK3m2wV"
   },
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# my_path = '/content/notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e-CzlEqVpJKF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv' , encoding = 'euc-kr')\n",
    "train_data = pd.read_csv('train_data.csv' , encoding = 'euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "2.12.0\n",
      "0.6.0\n",
      "1.5.3\n",
      "1.23.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import konlpy\n",
    "print(konlpy.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EBWwRLGJtjsH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loaded_model = load_model('/content/drive/MyDrive/data_review/best_model.h5')\n",
    "# print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 버전 확인\n",
    "# pip install --upgrade tensorflow==2.12.0\n",
    "# pip install --upgrade keras==2.12.0\n",
    "\n",
    "loaded_model = load_model('best_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2dJ8zAVjpJMP"
   },
   "outputs": [],
   "source": [
    "# 불용어를 지정하여 필요없는 토큰들은 제거하도록 합니다.\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', \n",
    "             '한', '에', '하', '고', '을', '를', '인', '듯', \n",
    "             '과', '와', '네', '들', '듯', '지', '임', '게']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xR-TI5mopNTV"
   },
   "outputs": [],
   "source": [
    "# 단어 집합(vocabulary)의 크기 : 64152\n",
    "# 등장 빈도가 1번 이하인 희귀 단어의 수: 29480\n",
    "# 단어 집합에서 희귀 단어의 비율: 45.953360768175585\n",
    "# 전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.289171484296422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9owopf3ra9-"
   },
   "outputs": [],
   "source": [
    "# total_cnt = 64152\n",
    "# rare_cnt = 29480\n",
    "# vocab_size = total_cnt - rare_cnt + 2\n",
    "# tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rOb_sTGkpNVt"
   },
   "outputs": [],
   "source": [
    "# 긍정 리뷰보다는 부정 리뷰가 좀 더 길게 작성된 경향이 있는 것 같습니다.\n",
    "\n",
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "omkolY5NqzNi"
   },
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "\n",
    "# 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. \n",
    "# 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다.\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6qzv4BkTpNX7"
   },
   "outputs": [],
   "source": [
    "total_cnt = 64152\n",
    "rare_cnt = 29480\n",
    "vocab_size = total_cnt - rare_cnt + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zX-NbvE6p3_P"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 단어 집합의 크기는 34674개입니다. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 이를 토크나이저의 인자로 넘겨주고, 텍스트 시퀀스를 정수 시퀀스로 변환합니다. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(vocab_size, oov_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOV\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 6\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:293\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 74\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     76\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[0;32m     77\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# 단어 집합의 크기는 34674개입니다. \n",
    "# 이를 토크나이저의 인자로 넘겨주고, 텍스트 시퀀스를 정수 시퀀스로 변환합니다. \n",
    "# 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다.\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9m7_66Jp4Bo"
   },
   "outputs": [],
   "source": [
    "max_len = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8LhKlZLqIl_"
   },
   "outputs": [],
   "source": [
    "# 훈련용 리뷰의 99.99%가 330 이하의 길이를 가집니다. 훈련용 리뷰를 길이 330으로 패딩하겠습니다.\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FvIwGYVmpJOY"
   },
   "outputs": [],
   "source": [
    "# # test파일 업로드\n",
    "\n",
    "# from google.colab import files\n",
    "\n",
    "# # 파일 선택 대화상자 표시\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# # 업로드한 파일 확인\n",
    "# for filename in uploaded.keys():\n",
    "#     print(f'Uploaded file: {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UzfRofBpshuu"
   },
   "outputs": [],
   "source": [
    "# test 파일 업로드 2\n",
    "#uploaded\n",
    "# test_re = pd.read_csv('/content/drive/MyDrive/(주)시더스초밥제주연동점_review.csv' , encoding = 'euc-kr')\n",
    "test_re = pd.read_csv('(주)시더스초밥제주연동점_review.csv' , encoding = 'euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VtcLqWBQsmBU"
   },
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "test_re = test_re.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uTJGQ3XMpJQh"
   },
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence)\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "\n",
    "    score = float(loaded_model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else :\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eR3Gie1isplT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "원코스요리\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(num)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msentiment_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m, in \u001b[0;36msentiment_predict\u001b[1;34m(new_sentence)\u001b[0m\n\u001b[0;32m      4\u001b[0m new_sentence \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m new_sentence \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stopwords]\n\u001b[0;32m      5\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([new_sentence])\n\u001b[1;32m----> 6\u001b[0m pad_new \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loaded_model\u001b[38;5;241m.\u001b[39mpredict(pad_new))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (score \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:1129\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m-> 1129\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for num , i in enumerate(test_re.리뷰.tolist()) :\n",
    "    print(num)\n",
    "    print(i)\n",
    "    print(sentiment_predict(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
