{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f226ad44-dc7c-4ea8-9d0c-6de9dd2b0a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "# import matplotlib.pyplot as plt\n",
    "# import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.models import load_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120abd2c-4076-4d94-9640-dd62d4402d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# import keras\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import h5py\n",
    "# import konlpy \n",
    "\n",
    "# print('tensorflow ',tensorflow.__version__)\n",
    "# print('keras', keras.__version__)\n",
    "# print('numpy', np.__version__)\n",
    "# print('padnas', pd.__version__)\n",
    "# print('h5py', h5py.__version__)\n",
    "# print('konlpy', konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6df72a5-79f2-4ddb-b03c-a1745e650a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class psng_predict_all:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        path = 'C:/Users/PiGiraffe0/Documents/triton-modeldata/modeldata'\n",
    "        self.loaded_model = load_model(f'{path}/new_model_01_v2.h5')\n",
    "        train_data = pd.read_csv(f'{path}/train_data_main.csv', encoding='euc-kr')\n",
    "        test_data = pd.read_csv(f'{path}/test_data_main.csv', encoding='euc-kr')\n",
    "        X_train = train_data['tokenized'].apply(eval).values\n",
    "        self.X_test = test_data['tokenized'].apply(eval).values\n",
    "        self.y_test = np.array(test_data['label'].values)\n",
    "        self.X_test_tkd = None\n",
    "        vocab_size = 87912 # change\n",
    "        self.tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        self.max_len = 250\n",
    "\n",
    "    def preprocessing(self):\n",
    "        okt = Okt()\n",
    "        R_frm = self.df.copy()\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "        R_frm['리뷰'].replace('', np.nan, inplace=True)\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
    "        stopwords = ['도', '는', '다', '의', '가', '이', '은',\n",
    "                     '한', '에', '하', '고', '을', '를', '인', '듯',\n",
    "                     '과', '와', '네', '들', '듯', '지', '임', '게',\n",
    "                     '는', '이', '했', '슴', '음', '것', '거', '로',\n",
    "                     '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
    "                     '수', '할', '그', '함', '돈', '번', '두', '개',\n",
    "                     '건', '내', '저', '만', '갈', '걸', '제', '명',\n",
    "                     '분',\n",
    "                     '인데', '이가', '했었', '해서',\n",
    "                     '습니다', '했는데', '입니다']\n",
    "\n",
    "        okt = Okt()\n",
    "        R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
    "        # R_frm['tokenized'] = R_frm['리뷰'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        R_frm['tokenized'] = R_frm['tokenized'].apply(\n",
    "            lambda x: [word for word, shape in x if shape in ['Verb', 'Adjective', 'Noun', 'VerbPrefix'] if word not in stopwords])\n",
    "        R_pred = R_frm['tokenized'].values\n",
    "        return R_pred\n",
    "\n",
    "    def model_test(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        X_test = tokenizer.texts_to_sequences(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "        print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "    def word_index(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        print(tokenizer.word_index)\n",
    "\n",
    "    def predict(self):\n",
    "        R_pred = self.preprocessing()\n",
    "        max_len = self.max_len\n",
    "        tokenizer = self.tokenizer\n",
    "        result = []\n",
    "        pred = tokenizer.texts_to_sequences(R_pred)\n",
    "        pred = pad_sequences(pred, maxlen=max_len)\n",
    "        score = self.loaded_model.predict(pred)\n",
    "        return score\n",
    "\n",
    "    def prediction(self):\n",
    "        result = []\n",
    "        score = self.predict()\n",
    "        for num in range(len(score)):\n",
    "            if score[num][0] > 0.9:\n",
    "                result.append('0')\n",
    "            elif score[num][0] < 0.1:\n",
    "                result.append('1')\n",
    "            else:\n",
    "                result.append('2')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d3d8b0-045d-447c-b0b6-8eb74377530e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class psng_predict_all_v2:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        path = ''\n",
    "        self.loaded_model = load_model(f'{path}posneg_no_oneword.h5')\n",
    "        train_data = pd.read_csv(f'{path}train_data_main_v2.csv', encoding='euc-kr')\n",
    "        test_data = pd.read_csv(f'{path}test_data_main_v2.csv', encoding='euc-kr')\n",
    "        X_train = train_data['tokenized'].apply(eval).values\n",
    "        self.X_test = test_data['tokenized'].apply(eval).values\n",
    "        self.y_test = np.array(test_data['label'].values)\n",
    "        self.X_test_tkd = None\n",
    "        vocab_size = 20894 # change\n",
    "        self.tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        self.max_len = 250\n",
    "\n",
    "    def preprocessing(self):\n",
    "        okt = Okt()\n",
    "        R_frm = self.df.copy()\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "        R_frm['리뷰'].replace('', np.nan, inplace=True)\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
    "        stopwords = ['도', '는', '다', '의', '가', '이', '은',\n",
    "                     '한', '에', '하', '고', '을', '를', '인', '듯',\n",
    "                     '과', '와', '네', '들', '듯', '지', '임', '게',\n",
    "                     '는', '이', '했', '슴', '음', '것', '거', '로',\n",
    "                     '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
    "                     '수', '할', '그', '함', '돈', '번', '두', '개',\n",
    "                     '건', '내', '저', '만', '갈', '걸', '제', '명',\n",
    "                     '분',\n",
    "                     '인데', '이가', '했었', '해서',\n",
    "                     '습니다', '했는데', '입니다']\n",
    "\n",
    "        okt = Okt()\n",
    "        # R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
    "        R_frm['tokenized'] = R_frm['리뷰'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        R_frm['tokenized'] = R_frm['tokenized'].apply(\n",
    "            lambda x: [word for word, shape in x if shape in ['Verb', 'Adjective', 'Noun', 'VerbPrefix'] if word not in stopwords])\n",
    "        R_pred = R_frm['tokenized'].values\n",
    "        return R_pred\n",
    "\n",
    "    def model_test(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        X_test = tokenizer.texts_to_sequences(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "        print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "    def word_index(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        print(tokenizer.word_index)\n",
    "\n",
    "    def predict(self):\n",
    "        R_pred = self.preprocessing()\n",
    "        max_len = self.max_len\n",
    "        tokenizer = self.tokenizer\n",
    "        result = []\n",
    "        pred = tokenizer.texts_to_sequences(R_pred)\n",
    "        pred = pad_sequences(pred, maxlen=max_len)\n",
    "        score = self.loaded_model.predict(pred)\n",
    "        return score\n",
    "\n",
    "    def prediction(self):\n",
    "        result = []\n",
    "        score = self.predict()\n",
    "        for num in range(len(score)):\n",
    "            if score[num][0] > 0.9:\n",
    "                result.append('0')\n",
    "            elif score[num][0] < 0.1:\n",
    "                result.append('1')\n",
    "            else:\n",
    "                result.append('2')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c395fa11-8aa6-4c57-bff1-e7d98eaee233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# youngho/nare\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "DSNNAME = \"Tibero6\"\n",
    "DBUSER = \"youngho\"\n",
    "DBPWD = \"nare\"\n",
    "\n",
    "cnxn = pyodbc.connect('DSN='+DSNNAME+';UID='+DBUSER+';PWD='+DBPWD)\n",
    "cursor = cnxn.cursor()\n",
    "cursor.execute(\"SELECT REST_ID , REVIEW , LABEL_TF from REVIEW_TF;\")\n",
    "\n",
    "# 1개 row만 가져오기\n",
    "# row = cursor.fetchone()\n",
    "# 모든 row 가져오기\n",
    "row2 = cursor.fetchall()\n",
    "# while row :\n",
    "#     print(row[0])\n",
    "#     row = cursor.fetchone()\n",
    "#     # row = cursor.fetchall()\n",
    "# print(row)\n",
    "\n",
    "cnxn.commit()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae8303bd-e7ed-4c1d-889d-17d756a45e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### youngho/nare\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "DSNNAME = \"Tibero6\"\n",
    "DBUSER = \"JH\"\n",
    "DBPWD = \"nare\"\n",
    "\n",
    "cnxn = pyodbc.connect('DSN='+DSNNAME+';UID='+DBUSER+';PWD='+DBPWD)\n",
    "cursor = cnxn.cursor()\n",
    "cursor.execute(\"SELECT REST_ID , REVIEW from youngho.REVIEW_TF;\")\n",
    "\n",
    "# 1개 row만 가져오기\n",
    "# row = cursor.fetchone()\n",
    "# 모든 row 가져오기\n",
    "row = cursor.fetchall()\n",
    "# while row :\n",
    "#     print(row[0])\n",
    "#     row = cursor.fetchone()\n",
    "#     # row = cursor.fetchall()\n",
    "# print(row)\n",
    "\n",
    "cnxn.commit()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d65cd74b-e5d8-4fab-b83f-12a55f41ad65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77509"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "608c9812-11c6-427e-a80e-4f9ac87564e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dccfa75a-4f9a-4023-86c1-383ffcd093b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      REST_ID                                             REVIEW LABEL_TF\n",
      "0  1713847517                                              너무맛있다        F\n",
      "1  1713847517                                     최고 제주시내 최고인것같음        T\n",
      "2  1713847517  분위기도 좋고\\n맛도 좋고\\n친절한 설명까지~!^^\\n행복한 시간이었습니다#\\n감사...        T\n",
      "3  1713847517  리뷰를 미리 남겨버렸네요 ?\\n분위기 좋고 친절한 직원분들덕에 제주도에서 분위기내고...        F\n",
      "4  1713847517  전 양고기스테이크 맛있었고 디저트 코코파인도 맛있었어요!!\\n소고기스테이크는 레어로...        T\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "row = [\n",
    "    ('1713847517', '너무맛있다', 'F'),\n",
    "    ('1713847517', '최고 제주시내 최고인것같음', 'T'),\n",
    "    ('1713847517', '분위기도 좋고\\n맛도 좋고\\n친절한 설명까지~!^^\\n행복한 시간이었습니다#\\n감사합니다~!', 'T'),\n",
    "    ('1713847517', '리뷰를 미리 남겨버렸네요 ?\\n분위기 좋고 친절한 직원분들덕에 제주도에서 분위기내고 식사할 수 있었어요~~^^', 'F'),\n",
    "    ('1713847517', '전 양고기스테이크 맛있었고 디저트 코코파인도 맛있었어요!!\\n소고기스테이크는 레어로 구워야 덜 질길 것 같아요! 스테이크 다 미디움레어로 했는데 소고기는 좀 많이 익은 느낌이었고 오리는 보통 양고기는 딱 좋았어요!!\\n같이 왔던 지인은 오리고기스테이크가 제일 특별하고 맛있었대요!! 어디가서 먹어보기 힘든 음식이라고 맛있었다고 하더라구요~~\\n그리고 직원분이 와인 설명도 잘 해주시고 나중에 대리도 불러주시고 엄청 친절하셨어요 감사합니다~\\n버섯 리조또(?)가 평이 엄청 좋아서 다음에는 버섯 리조또랑 뇨끼 먹어보고 싶어요 ㅎㅎ\\n분위기도 좋고 맛도 있어서 제주도에서 특별한 저녁식사 하고가용 ☺️', 'T')\n",
    "]\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(row, columns=['REST_ID', 'REVIEW', 'LABEL_TF'])\n",
    "\n",
    "# 결과 출력\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa2065d4-620b-4384-b227-ed2b784d5fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = row\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f356328-334d-4cc7-b85c-a1a4718f187a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst_id = []\n",
    "lst_review = []\n",
    "for i, r in row :\n",
    "    lst_id.append(i)\n",
    "    lst_review.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4da63ad6-85a4-4313-9735-e4418c40b271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_frm = pd.DataFrame({'REST_ID' : lst_id , 'REVIEW' : lst_review})\n",
    "row_json = row_frm[['REVIEW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3dcec5e-71b7-46d5-a990-ae910703c63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_frm = pd.DataFrame({'REST_ID' : lst_id , 'REVIEW' : lst_review})\n",
    "row_json = row_frm[['REVIEW']]\n",
    "json_row = row_json.to_json(orient='records', force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8941d9db-54f1-4163-8046-ca743cfc99b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# json_data = json.loads(json_row)\n",
    "# for review in json_data :\n",
    "#     print(review['REVIEW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1245ca0-ee1a-4c51-aa64-2d9caef365dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class psng_predict_all_v2:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        path = ''\n",
    "        self.loaded_model = load_model(f'{path}posneg_no_oneword.h5')\n",
    "        train_data = pd.read_csv(f'{path}train_data_main_v2.csv', encoding='euc-kr')\n",
    "        \n",
    "        X_train = train_data['tokenized'].apply(eval).values\n",
    "        vocab_size = 20894 # change\n",
    "        self.tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        self.max_len = 250\n",
    "\n",
    "    def preprocessing(self):\n",
    "        okt = Okt()\n",
    "        R_frm = self.df.copy()\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "        R_frm['리뷰'].replace('', np.nan, inplace=True)\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
    "        stopwords = ['도', '는', '다', '의', '가', '이', '은',\n",
    "                     '한', '에', '하', '고', '을', '를', '인', '듯',\n",
    "                     '과', '와', '네', '들', '듯', '지', '임', '게',\n",
    "                     '는', '이', '했', '슴', '음', '것', '거', '로',\n",
    "                     '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
    "                     '수', '할', '그', '함', '돈', '번', '두', '개',\n",
    "                     '건', '내', '저', '만', '갈', '걸', '제', '명',\n",
    "                     '분',\n",
    "                     '인데', '이가', '했었', '해서',\n",
    "                     '습니다', '했는데', '입니다']\n",
    "\n",
    "        okt = Okt()\n",
    "        # R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
    "        R_frm['tokenized'] = R_frm['리뷰'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        R_frm['tokenized'] = R_frm['tokenized'].apply(\n",
    "            lambda x: [word for word, shape in x if shape in ['Verb', 'Adjective', 'Noun', 'VerbPrefix'] if word not in stopwords])\n",
    "        R_pred = R_frm['tokenized'].values\n",
    "        return R_pred\n",
    "\n",
    "    def model_test(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        X_test = tokenizer.texts_to_sequences(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "        print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "    def word_index(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        print(tokenizer.word_index)\n",
    "\n",
    "    def predict(self):\n",
    "        R_pred = self.preprocessing()\n",
    "        max_len = self.max_len\n",
    "        tokenizer = self.tokenizer\n",
    "        result = []\n",
    "        pred = tokenizer.texts_to_sequences(R_pred)\n",
    "        pred = pad_sequences(pred, maxlen=max_len)\n",
    "        score = self.loaded_model.predict(pred)\n",
    "        return score\n",
    "\n",
    "    def prediction(self):\n",
    "        result = []\n",
    "        score = self.predict()\n",
    "        for num in range(len(score)):\n",
    "            if score[num][0] > 0.9:\n",
    "                result.append('0')\n",
    "            elif score[num][0] < 0.1:\n",
    "                result.append('1')\n",
    "            else:\n",
    "                result.append('2')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fbf56-25b5-40ad-90df-7d3b5fd03234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
