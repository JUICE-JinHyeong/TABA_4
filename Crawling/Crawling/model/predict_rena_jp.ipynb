{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f226ad44-dc7c-4ea8-9d0c-6de9dd2b0a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "# import matplotlib.pyplot as plt\n",
    "# import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120abd2c-4076-4d94-9640-dd62d4402d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# import keras\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import h5py\n",
    "# import konlpy \n",
    "\n",
    "# print('tensorflow ',tensorflow.__version__)\n",
    "# print('keras', keras.__version__)\n",
    "# print('numpy', np.__version__)\n",
    "# print('padnas', pd.__version__)\n",
    "# print('h5py', h5py.__version__)\n",
    "# print('konlpy', konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6df72a5-79f2-4ddb-b03c-a1745e650a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class psng_predict_all:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        path = 'C:/Users/PiGiraffe0/Documents/triton-modeldata/modeldata'\n",
    "        self.loaded_model = load_model(f'{path}/new_model_01_v2.h5')\n",
    "        train_data = pd.read_csv(f'{path}/train_data_main.csv', encoding='euc-kr')\n",
    "        test_data = pd.read_csv(f'{path}/test_data_main.csv', encoding='euc-kr')\n",
    "        X_train = train_data['tokenized'].apply(eval).values\n",
    "        self.X_test = test_data['tokenized'].apply(eval).values\n",
    "        self.y_test = np.array(test_data['label'].values)\n",
    "        self.X_test_tkd = None\n",
    "        vocab_size = 87912 # change\n",
    "        self.tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        self.max_len = 250\n",
    "\n",
    "    def preprocessing(self):\n",
    "        okt = Okt()\n",
    "        R_frm = self.df.copy()\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "        R_frm['리뷰'].replace('', np.nan, inplace=True)\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
    "        stopwords = ['도', '는', '다', '의', '가', '이', '은',\n",
    "                     '한', '에', '하', '고', '을', '를', '인', '듯',\n",
    "                     '과', '와', '네', '들', '듯', '지', '임', '게',\n",
    "                     '는', '이', '했', '슴', '음', '것', '거', '로',\n",
    "                     '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
    "                     '수', '할', '그', '함', '돈', '번', '두', '개',\n",
    "                     '건', '내', '저', '만', '갈', '걸', '제', '명',\n",
    "                     '분',\n",
    "                     '인데', '이가', '했었', '해서',\n",
    "                     '습니다', '했는데', '입니다']\n",
    "\n",
    "        okt = Okt()\n",
    "        R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
    "        # R_frm['tokenized'] = R_frm['리뷰'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        R_frm['tokenized'] = R_frm['tokenized'].apply(\n",
    "            lambda x: [word for word, shape in x if shape in ['Verb', 'Adjective', 'Noun', 'VerbPrefix'] if word not in stopwords])\n",
    "        R_pred = R_frm['tokenized'].values\n",
    "        return R_pred\n",
    "\n",
    "    def model_test(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        X_test = tokenizer.texts_to_sequences(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "        print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "    def word_index(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        print(tokenizer.word_index)\n",
    "\n",
    "    def predict(self):\n",
    "        R_pred = self.preprocessing()\n",
    "        max_len = self.max_len\n",
    "        tokenizer = self.tokenizer\n",
    "        result = []\n",
    "        pred = tokenizer.texts_to_sequences(R_pred)\n",
    "        pred = pad_sequences(pred, maxlen=max_len)\n",
    "        score = self.loaded_model.predict(pred)\n",
    "        return score\n",
    "\n",
    "    def prediction(self):\n",
    "        result = []\n",
    "        score = self.predict()\n",
    "        for num in range(len(score)):\n",
    "            if score[num][0] > 0.9:\n",
    "                result.append('0')\n",
    "            elif score[num][0] < 0.1:\n",
    "                result.append('1')\n",
    "            else:\n",
    "                result.append('2')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d3d8b0-045d-447c-b0b6-8eb74377530e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class psng_predict_all_v2:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        path = ''\n",
    "        self.loaded_model = load_model(f'{path}posneg_no_oneword.h5')\n",
    "        train_data = pd.read_csv(f'{path}train_data_main_v2.csv', encoding='euc-kr')\n",
    "        test_data = pd.read_csv(f'{path}test_data_main_v2.csv', encoding='euc-kr')\n",
    "        X_train = train_data['tokenized'].apply(eval).values\n",
    "        self.X_test = test_data['tokenized'].apply(eval).values\n",
    "        self.y_test = np.array(test_data['label'].values)\n",
    "        self.X_test_tkd = None\n",
    "        vocab_size = 20894 # change\n",
    "        self.tokenizer = Tokenizer(vocab_size, oov_token='OOV')\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        self.max_len = 250\n",
    "\n",
    "    def preprocessing(self):\n",
    "        okt = Okt()\n",
    "        R_frm = self.df.copy()\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n",
    "        R_frm['리뷰'].replace('', np.nan, inplace=True)\n",
    "        R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
    "        stopwords = ['도', '는', '다', '의', '가', '이', '은',\n",
    "                     '한', '에', '하', '고', '을', '를', '인', '듯',\n",
    "                     '과', '와', '네', '들', '듯', '지', '임', '게',\n",
    "                     '는', '이', '했', '슴', '음', '것', '거', '로',\n",
    "                     '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
    "                     '수', '할', '그', '함', '돈', '번', '두', '개',\n",
    "                     '건', '내', '저', '만', '갈', '걸', '제', '명',\n",
    "                     '분',\n",
    "                     '인데', '이가', '했었', '해서',\n",
    "                     '습니다', '했는데', '입니다']\n",
    "\n",
    "        okt = Okt()\n",
    "        # R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
    "        R_frm['tokenized'] = R_frm['리뷰'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "        R_frm['tokenized'] = R_frm['tokenized'].apply(\n",
    "            lambda x: [word for word, shape in x if shape in ['Verb', 'Adjective', 'Noun', 'VerbPrefix'] if word not in stopwords])\n",
    "        R_pred = R_frm['tokenized'].values\n",
    "        return R_pred\n",
    "\n",
    "    def model_test(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        X_test = tokenizer.texts_to_sequences(self.X_test)\n",
    "        y_test = self.y_test\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "        print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "    def word_index(self):\n",
    "        tokenizer = self.tokenizer\n",
    "        print(tokenizer.word_index)\n",
    "\n",
    "    def predict(self):\n",
    "        R_pred = self.preprocessing()\n",
    "        max_len = self.max_len\n",
    "        tokenizer = self.tokenizer\n",
    "        result = []\n",
    "        pred = tokenizer.texts_to_sequences(R_pred)\n",
    "        pred = pad_sequences(pred, maxlen=max_len)\n",
    "        score = self.loaded_model.predict(pred)\n",
    "        return score\n",
    "\n",
    "    def prediction(self):\n",
    "        result = []\n",
    "        score = self.predict()\n",
    "        for num in range(len(score)):\n",
    "            if score[num][0] > 0.9:\n",
    "                result.append('0')\n",
    "            elif score[num][0] < 0.1:\n",
    "                result.append('1')\n",
    "            else:\n",
    "                result.append('2')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fbf56-25b5-40ad-90df-7d3b5fd03234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
