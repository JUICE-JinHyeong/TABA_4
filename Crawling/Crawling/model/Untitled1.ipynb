{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab8017-3a13-4ef0-917e-80f75c91089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "import numpy as np\n",
    "\n",
    "# Triton Inference Server의 주소와 포트를 설정합니다.\n",
    "server_url = 'localhost:8001'\n",
    "\n",
    "# 추론에 사용할 모델의 이름과 버전을 설정합니다.\n",
    "model_name = 'psng_predictor'\n",
    "model_version = '1'\n",
    "\n",
    "# 입력 데이터를 설정합니다.\n",
    "input_data = np.random.rand(1, 250)  # 입력 데이터를 적절한 형태로 준비합니다.\n",
    "\n",
    "# Create gRPC stub for communicating with the server\n",
    "channel = grpc.insecure_channel(server_url)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# 모델 추론 요청 생성\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = model_name\n",
    "request.model_version = model_version\n",
    "\n",
    "# 입력 텐서 설정\n",
    "input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "input_tensor.name = 'input_tensor'  # 입력 텐서의 이름 설정\n",
    "input_tensor.datatype = 'FP32'  # 입력 데이터의 데이터 타입 설정\n",
    "input_tensor.shape.extend([1, 250])  # 입력 텐서의 형태 설정\n",
    "input_tensor.contents.raw_contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "request.inputs.extend([input_tensor])\n",
    "\n",
    "# 모델 추론 요청 보내기\n",
    "response = grpc_stub.ModelInfer(request)\n",
    "\n",
    "# 결과 처리\n",
    "if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "    print(f'Inference failed with error message: {response.error.message}')\n",
    "else:\n",
    "    # 추론 결과 처리\n",
    "    output_data = response.outputs[0].contents.raw_contents\n",
    "    output_data = np.frombuffer(output_data, dtype=np.float32)\n",
    "    output_data = output_data.reshape((1, -1))\n",
    "    print(f'Inference result: {output_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36bbf1a-9499-49db-bc02-f3f24b555436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Protocol message InferTensorContents has no \"raw_contents\" field.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m inference \u001b[38;5;241m=\u001b[39m TritonModelInference(server_url, model_name, model_version)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 모델 추론 실행\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mTritonModelInference.infer\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     23\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdatatype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP32\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 입력 데이터의 데이터 타입 설정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m])  \u001b[38;5;66;03m# 입력 텐서의 형태 설정\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mraw_contents \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtobytes()  \u001b[38;5;66;03m# 입력 데이터 설정\u001b[39;00m\n\u001b[0;32m     27\u001b[0m request\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mextend([input_tensor])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 모델 추론 요청 보내기\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Protocol message InferTensorContents has no \"raw_contents\" field."
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "import numpy as np\n",
    "\n",
    "class TritonModelInference:\n",
    "    def __init__(self, server_url, model_name, model_version):\n",
    "        self.server_url = server_url\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.channel = grpc.insecure_channel(server_url)\n",
    "        self.grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(self.channel)\n",
    "\n",
    "    def infer(self, input_data):\n",
    "        # 모델 추론 요청 생성\n",
    "        request = service_pb2.ModelInferRequest()\n",
    "        request.model_name = self.model_name\n",
    "        request.model_version = self.model_version\n",
    "\n",
    "        # 입력 텐서 설정\n",
    "        input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "        input_tensor.name = 'input_tensor'  # 입력 텐서의 이름 설정\n",
    "        input_tensor.datatype = 'FP32'  # 입력 데이터의 데이터 타입 설정\n",
    "        input_tensor.shape.extend([1, 250])  # 입력 텐서의 형태 설정\n",
    "        input_tensor.contents.raw_contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "        request.inputs.extend([input_tensor])\n",
    "\n",
    "        # 모델 추론 요청 보내기\n",
    "        response = self.grpc_stub.ModelInfer(request)\n",
    "\n",
    "        # 결과 처리\n",
    "        if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "            print(f'Inference failed with error message: {response.error.message}')\n",
    "            return None\n",
    "        else:\n",
    "            # 추론 결과 처리\n",
    "            output_data = response.outputs[0].contents.raw_contents\n",
    "            output_data = np.frombuffer(output_data, dtype=np.float32)\n",
    "            output_data = output_data.reshape((1, -1))\n",
    "            return output_data\n",
    "\n",
    "# Triton Inference Server의 주소와 포트를 설정합니다.\n",
    "server_url = 'localhost:8001'\n",
    "\n",
    "# 추론에 사용할 모델의 이름과 버전을 설정합니다.\n",
    "model_name = 'psng_predictor'\n",
    "model_version = '1'\n",
    "\n",
    "# 입력 데이터를 설정합니다.\n",
    "input_data = np.random.rand(1, 250)  # 입력 데이터를 적절한 형태로 준비합니다.\n",
    "\n",
    "# TritonModelInference 객체 생성\n",
    "inference = TritonModelInference(server_url, model_name, model_version)\n",
    "\n",
    "# 모델 추론 실행\n",
    "output_data = inference.infer(input_data)\n",
    "\n",
    "if output_data is not None:\n",
    "    print(f'Inference result: {output_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c3d4e3-a6d2-427f-bbc7-833acc0aa2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Assignment not allowed to message, map, or repeated field \"contents\" in protocol message object.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m inference \u001b[38;5;241m=\u001b[39m TritonModelInference(server_url, model_name, model_version)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 모델 추론 실행\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mTritonModelInference.infer\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     23\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdatatype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP32\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 입력 데이터의 데이터 타입 설정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m])  \u001b[38;5;66;03m# 입력 텐서의 형태 설정\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtobytes()  \u001b[38;5;66;03m# 입력 데이터 설정\u001b[39;00m\n\u001b[0;32m     27\u001b[0m request\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mextend([input_tensor])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 모델 추론 요청 보내기\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Assignment not allowed to message, map, or repeated field \"contents\" in protocol message object."
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "import numpy as np\n",
    "\n",
    "class TritonModelInference:\n",
    "    def __init__(self, server_url, model_name, model_version):\n",
    "        self.server_url = server_url\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.channel = grpc.insecure_channel(server_url)\n",
    "        self.grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(self.channel)\n",
    "\n",
    "    def infer(self, input_data):\n",
    "        # 모델 추론 요청 생성\n",
    "        request = service_pb2.ModelInferRequest()\n",
    "        request.model_name = self.model_name\n",
    "        request.model_version = self.model_version\n",
    "\n",
    "        # 입력 텐서 설정\n",
    "        input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "        input_tensor.name = 'input_tensor'  # 입력 텐서의 이름 설정\n",
    "        input_tensor.datatype = 'FP32'  # 입력 데이터의 데이터 타입 설정\n",
    "        input_tensor.shape.extend([1, 250])  # 입력 텐서의 형태 설정\n",
    "        input_tensor.contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "        request.inputs.extend([input_tensor])\n",
    "\n",
    "        # 모델 추론 요청 보내기\n",
    "        response = self.grpc_stub.ModelInfer(request)\n",
    "\n",
    "        # 결과 처리\n",
    "        if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "            print(f'Inference failed with error message: {response.error.message}')\n",
    "            return None\n",
    "        else:\n",
    "            # 추론 결과 처리\n",
    "            output_data = response.outputs[0].contents\n",
    "            output_data = np.frombuffer(output_data, dtype=np.float32)\n",
    "            output_data = output_data.reshape((1, -1))\n",
    "            return output_data\n",
    "\n",
    "# Triton Inference Server의 주소와 포트를 설정합니다.\n",
    "server_url = 'localhost:8001'\n",
    "\n",
    "# 추론에 사용할 모델의 이름과 버전을 설정합니다.\n",
    "model_name = 'psng_predictor'\n",
    "model_version = '1'\n",
    "\n",
    "# 입력 데이터를 설정합니다.\n",
    "input_data = np.random.rand(1, 250)  # 입력 데이터를 적절한 형태로 준비합니다.\n",
    "\n",
    "# TritonModelInference 객체 생성\n",
    "inference = TritonModelInference(server_url, model_name, model_version)\n",
    "\n",
    "# 모델 추론 실행\n",
    "output_data = inference.infer(input_data)\n",
    "\n",
    "if output_data is not None:\n",
    "    print(f'Inference result: {output_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a290a002-53bd-409c-92b8-925595dc620d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tritonclient.grpc.service_pb2' has no attribute 'TYPE_FP32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m inference \u001b[38;5;241m=\u001b[39m TritonModelInference(server_url, model_name, model_version)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 모델 추론 실행\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mTritonModelInference.infer\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     21\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39madd()\n\u001b[0;32m     22\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 입력 텐서의 이름 설정\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m=\u001b[39m \u001b[43mservice_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTYPE_FP32\u001b[49m  \u001b[38;5;66;03m# 입력 데이터의 데이터 타입 설정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m])  \u001b[38;5;66;03m# 입력 텐서의 형태 설정\u001b[39;00m\n\u001b[0;32m     25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mraw_contents \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtobytes()  \u001b[38;5;66;03m# 입력 데이터 설정\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tritonclient.grpc.service_pb2' has no attribute 'TYPE_FP32'"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "import numpy as np\n",
    "\n",
    "class TritonModelInference:\n",
    "    def __init__(self, server_url, model_name, model_version):\n",
    "        self.server_url = server_url\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.channel = grpc.insecure_channel(server_url)\n",
    "        self.grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(self.channel)\n",
    "\n",
    "    def infer(self, input_data):\n",
    "        # 모델 추론 요청 생성\n",
    "        request = service_pb2.ModelInferRequest()\n",
    "        request.model_name = self.model_name\n",
    "        request.model_version = self.model_version\n",
    "\n",
    "        # 입력 텐서 설정\n",
    "        input_tensor = request.inputs.add()\n",
    "        input_tensor.name = 'input_tensor'  # 입력 텐서의 이름 설정\n",
    "        input_tensor.data_type = service_pb2.TYPE_FP32  # 입력 데이터의 데이터 타입 설정\n",
    "        input_tensor.shape.extend([1, 250])  # 입력 텐서의 형태 설정\n",
    "        input_tensor.contents.raw_contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "        # 모델 추론 요청 보내기\n",
    "        response = self.grpc_stub.ModelInfer(request)\n",
    "\n",
    "        # 결과 처리\n",
    "        if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "            print(f'Inference failed with error message: {response.error.message}')\n",
    "            return None\n",
    "        else:\n",
    "            # 추론 결과 처리\n",
    "            output_data = response.outputs[0].contents.raw_contents\n",
    "            output_data = np.frombuffer(output_data, dtype=np.float32)\n",
    "            output_data = output_data.reshape((1, -1))\n",
    "            return output_data\n",
    "\n",
    "# Triton Inference Server의 주소와 포트를 설정합니다.\n",
    "server_url = 'localhost:8001'\n",
    "\n",
    "# 추론에 사용할 모델의 이름과 버전을 설정합니다.\n",
    "model_name = 'psng_predictor'\n",
    "model_version = '1'\n",
    "\n",
    "# 입력 데이터를 설정합니다.\n",
    "input_data = np.random.rand(1, 250)  # 입력 데이터를 적절한 형태로 준비합니다.\n",
    "\n",
    "# TritonModelInference 객체 생성\n",
    "inference = TritonModelInference(server_url, model_name, model_version)\n",
    "\n",
    "# 모델 추론 실행\n",
    "output_data = inference.infer(input_data)\n",
    "\n",
    "if output_data is not None:\n",
    "    print(f'Inference result: {output_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f452d8-3015-4f2e-b9b1-099a03f4bbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Assignment not allowed to message, map, or repeated field \"contents\" in protocol message object.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m inference \u001b[38;5;241m=\u001b[39m TritonModelInference(server_url, model_name, model_version)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 모델 추론 실행\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mTritonModelInference.infer\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     23\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdatatype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP32\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 입력 데이터의 데이터 타입 설정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m])  \u001b[38;5;66;03m# 입력 텐서의 형태 설정\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtobytes()  \u001b[38;5;66;03m# 입력 데이터 설정\u001b[39;00m\n\u001b[0;32m     27\u001b[0m request\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mextend([input_tensor])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 모델 추론 요청 보내기\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Assignment not allowed to message, map, or repeated field \"contents\" in protocol message object."
     ]
    }
   ],
   "source": [
    "# 이 코드에서 발생하는 AttributeError는 Protobuf 메시지의 repeated 필드에 직접 값을 할당하려고 시도했기 때문입니다. Protobuf에서 리스트 필드는 직접 할당하는 대신 append()나 extend()와 같은 메서드를 사용해야 합니다.\n",
    "\n",
    "# 여기서는 입력 데이터를 bytes 형태로 변환한 후 이를 입력 텐서의 내용(contents) 필드에 할당하려고 시도했습니다. 그러나 이는 Protobuf 메시지 필드에 직접 값을 할당하려는 시도이므로 오류가 발생했습니다.\n",
    "\n",
    "# 'contents' 필드는 원시 데이터(raw data)를 저장하는 데 사용되며, 이는 여러 형태(단일 값, 배열 등)의 데이터를 저장할 수 있습니다. 따라서 'contents' 필드에는 'raw_contents' 메서드를 사용해 값을 할당해야 합니다.\n",
    "\n",
    "# 그러므로 `input_tensor.contents = input_data.tobytes()` 라인을 아래와 같이 수정하면 됩니다:\n",
    "# input_tensor.raw_contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "\n",
    "# 수정 후 코드는 다음과 같습니다:\n",
    "def infer(self, input_data):\n",
    "    # 모델 추론 요청 생성\n",
    "    request = service_pb2.ModelInferRequest()\n",
    "    request.model_name = self.model_name\n",
    "    request.model_version = self.model_version\n",
    "\n",
    "    # 입력 텐서 설정\n",
    "    input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "    input_tensor.name = 'input_tensor'  # 입력 텐서의 이름 설정\n",
    "    input_tensor.datatype = 'FP32'  # 입력 데이터의 데이터 타입 설정\n",
    "    input_tensor.shape.extend([1, 250])  # 입력 텐서의 형태 설정\n",
    "    input_tensor.raw_contents = input_data.tobytes()  # 입력 데이터 설정\n",
    "\n",
    "    request.inputs.extend([input_tensor])\n",
    "\n",
    "    # 모델 추론 요청 보내기\n",
    "    response = self.grpc_stub.ModelInfer(request)\n",
    "\n",
    "    # 결과 처리\n",
    "    if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "        print(f'Inference failed with error message: {response.error.message}')\n",
    "        return None\n",
    "    else:\n",
    "        # 추론 결과 처리\n",
    "        output_data = response.outputs[0].contents\n",
    "        output_data = np.frombuffer(output_data, dtype=np.float32)\n",
    "        output_data = output_data.reshape((1, -1))\n",
    "        return output_data\n",
    "# Triton Inference Server의 주소와 포트를 설정합니다.\n",
    "server_url = 'localhost:8001'\n",
    "\n",
    "# 추론에 사용할 모델의 이름과 버전을 설정합니다.\n",
    "model_name = 'psng_predictor'\n",
    "model_version = '1'\n",
    "\n",
    "# 입력 데이터를 설정합니다.\n",
    "input_data = np.random.rand(1, 250)  # 입력 데이터를 적절한 형태로 준비합니다.\n",
    "\n",
    "# TritonModelInference 객체 생성\n",
    "inference = TritonModelInference(server_url, model_name, model_version)\n",
    "\n",
    "# 모델 추론 실행\n",
    "output_data = inference.infer(input_data)\n",
    "\n",
    "if output_data is not None:\n",
    "    print(f'Inference result: {output_data}')\n",
    "# 제가 제안한 수정이 코드의 오류를 해결해야 하지만, 트라이튼 인퍼런스 서버와의 통신이나 요청/응답 패킷의 특정 세부 사항에 따라 추가적인 조정이 필요할 수 있습니다. 위의 수정 후에도 문제가 지속되면 더 자세한 오류 메시지나 상황을 알려주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90907c8-cbdd-4f95-bdcb-6d0179c225cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3da1d9f-8091-49e4-a84f-8e40df756da1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PiGiraffe0\\anaconda3\\lib\\site-packages\\tritonhttpclient\\__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The installation does not include http support. Specify 'http' or 'all' while installing the tritonclient package to include the support",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tritonclient\\http\\__init__.py:29\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferAsyncRequest, InferenceServerClient\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_infer_input\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferInput\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tritonclient\\http\\_client.py:26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Redistribution and use in source and binary forms, with or without\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgevent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gevent'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtritonhttpclient\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 추론 요청을 보낼 클라이언트 생성\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tritonhttpclient\\__init__.py:36\u001b[0m\n\u001b[0;32m     29\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe package `tritonhttpclient` is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved in a future version. Please use instead \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tritonclient.http`\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tritonclient\\http\\__init__.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceServerClientPlugin\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe installation does not include http support. Specify \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mall\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m while installing the tritonclient package to include the support\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     38\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The installation does not include http support. Specify 'http' or 'all' while installing the tritonclient package to include the support"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "import numpy as np\n",
    "\n",
    "# 추론 요청을 보낼 클라이언트 생성\n",
    "client = tritonhttpclient.InferenceServerClient(url=\"http://localhost:8000\")\n",
    "\n",
    "# 입력 데이터 준비\n",
    "input_data = np.random.rand(1, 250)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "input_tensor = tritonhttpclient.InferInput(\"input_tensor\", input_data.shape, \"FP32\")\n",
    "input_tensor.set_data_from_numpy(input_data)\n",
    "\n",
    "# 추론 요청 생성\n",
    "request = client.infer(\"model_name\", model_version=\"1\", inputs=[input_tensor])\n",
    "\n",
    "# 추론 결과 확인\n",
    "output_tensor = request.get_output(\"output_tensor\")\n",
    "output_data = output_tensor.as_numpy()\n",
    "\n",
    "print(\"추론 결과:\", output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d32f0ab-ea5d-4f3b-a4b4-ccc533fa5ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tritonclient.grpc.service_pb2_grpc' has no attribute 'ModelInferRequest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m stub \u001b[38;5;241m=\u001b[39m service_pb2_grpc\u001b[38;5;241m.\u001b[39mGRPCInferenceServiceStub(channel)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 추론 요청 생성\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[43mservice_pb2_grpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelInferRequest\u001b[49m()\n\u001b[0;32m     12\u001b[0m request\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m request\u001b[38;5;241m.\u001b[39mmodel_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tritonclient.grpc.service_pb2_grpc' has no attribute 'ModelInferRequest'"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "# gRPC 서버에 연결\n",
    "channel = grpc.insecure_channel('localhost:8001')\n",
    "stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# 추론 요청 생성\n",
    "request = service_pb2_grpc.ModelInferRequest()\n",
    "request.model_name = \"model_name\"\n",
    "request.model_version = \"1\"\n",
    "\n",
    "# 입력 데이터 준비\n",
    "input_data = np.random.rand(1, 250)\n",
    "\n",
    "# 입력 텐서 설정\n",
    "input_tensor = service_pb2_grpc.ModelInferTensor()\n",
    "input_tensor.name = \"input_tensor\"\n",
    "input_tensor.shape.extend(input_data.shape)\n",
    "input_tensor.datatype = \"FP32\"\n",
    "input_tensor.contents_raw = input_data.tobytes()\n",
    "\n",
    "request.inputs.extend([input_tensor])\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.ModelInfer(request)\n",
    "\n",
    "# 추론 결과 확인\n",
    "output_tensor = response.outputs[0]\n",
    "output_data = np.frombuffer(output_tensor.contents_raw, dtype=np.float32)\n",
    "output_data = output_data.reshape(output_tensor.shape)\n",
    "\n",
    "print(\"추론 결과:\", output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd0abd5-c91e-451d-a82f-9c51e92f19a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbca0c-c1a6-471d-b408-3e4e2f0ddb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class TritonPythonModel:\n",
    "    def initialize(self, args):\n",
    "        \"\"\"모델 초기화 함수\"\"\"\n",
    "        print('Initialized...')\n",
    "        self.loaded_model = load_model('posneg_no_oneword_v3.h5')\n",
    "\n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "\n",
    "        for request in requests:\n",
    "            input_tensor = pb_utils.get_input_tensor_by_name(request, 'input_tensor')\n",
    "            input_data = input_tensor.as_numpy()\n",
    "\n",
    "            # 모델 추론 실행\n",
    "            output_data = self.loaded_model.predict(input_data)\n",
    "\n",
    "            # 결과를 새로운 텐서로 저장\n",
    "            output_tensor = pb_utils.Tensor(output_data)\n",
    "\n",
    "            # 추론 응답 생성\n",
    "            response = pb_utils.InferenceResponse(output_tensors=[output_tensor])\n",
    "            responses.append(response)\n",
    "\n",
    "        return responses\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"모델 종료 함수\"\"\"\n",
    "        print('Cleaning up...')\n",
    "\n",
    "# 모델 객체 생성\n",
    "model = TritonPythonModel()\n",
    "\n",
    "# 모델을 Triton Inference Server에 등록\n",
    "pb_utils.run(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77c27b-fa38-475b-8550-62bab4492b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: \"psng_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    "  {\n",
    "    name: \"input_tensor\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 250]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_tensor\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 1]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9911f8e-3a0f-47cc-b48b-95d5fd551d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6108bb08-a239-459c-99a3-33612ae7a113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad argument type for built-in operation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39madd()\n\u001b[0;32m     25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 26\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdatatype \u001b[38;5;241m=\u001b[39m model_config_pb2\u001b[38;5;241m.\u001b[39mTYPE_FP32\n\u001b[0;32m     27\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m])\n\u001b[0;32m     28\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Replace with your actual input data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: bad argument type for built-in operation"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "from tritonclient.grpc import model_config_pb2\n",
    "\n",
    "# Set Triton Inference Server's URL\n",
    "url = 'localhost:8001'\n",
    "\n",
    "# Set model information\n",
    "model_name = 'psng_model'\n",
    "model_version = ''\n",
    "\n",
    "# Create gRPC stub for communicating with the server\n",
    "channel = grpc.insecure_channel(url)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# Create inference request\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = model_name\n",
    "request.model_version = model_version\n",
    "\n",
    "# Set input tensor\n",
    "input_tensor = request.inputs.add()\n",
    "input_tensor.name = 'input_tensor'\n",
    "input_tensor.datatype = model_config_pb2.TYPE_FP32\n",
    "input_tensor.shape.extend([1, 250])\n",
    "input_data = np.random.rand(1, 250).astype(np.float32)  # Replace with your actual input data\n",
    "input_tensor.contents.raw_contents = input_data.tobytes()\n",
    "\n",
    "# Set output tensor\n",
    "output_tensor = request.outputs.add()\n",
    "output_tensor.name = 'output_tensor'\n",
    "output_tensor.datatype = model_config_pb2.TYPE_FP32\n",
    "output_tensor.shape.extend([1, 1])\n",
    "\n",
    "# Send inference request\n",
    "response = grpc_stub.ModelInfer(request)\n",
    "\n",
    "# Process inference response\n",
    "if response.error.code != service_pb2.ErrorResponse.SUCCESS:\n",
    "    print(f'Inference failed with error message: {response.error.message}')\n",
    "else:\n",
    "    output_data = response.outputs[0].contents.raw_contents\n",
    "    output_data = np.frombuffer(output_data, dtype=np.float32).reshape((1, 1))\n",
    "    print(f'Inference result: {output_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2753e-e3b7-4b5f-8d43-cfb6b59647f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83118d7-a829-439e-8ec4-0f01498bc9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tritonclient.grpc.service_pb2' has no attribute 'DataType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m250\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#input_tensor.data_type = service_pb2.TYPE_FP32\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m=\u001b[39m \u001b[43mservice_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataType\u001b[49m\u001b[38;5;241m.\u001b[39mTYPE_FP32\n\u001b[0;32m     25\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mraw_contents \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 출력 텐서 설정\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tritonclient.grpc.service_pb2' has no attribute 'DataType'"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "# gRPC 서버 URL 설정\n",
    "url = 'localhost:8001'\n",
    "\n",
    "# gRPC 채널 생성\n",
    "channel = grpc.insecure_channel(url)\n",
    "stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# 추론 요청 설정\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = 'psng_predictor'\n",
    "request.model_version = \"1\"\n",
    "\n",
    "# 입력 텐서 설정\n",
    "input_tensor = request.inputs.add()\n",
    "input_tensor.name = 'input_tensor'\n",
    "input_tensor.shape.extend([1, 250])\n",
    "input_data = np.random.rand(1, 250).astype(np.float32)\n",
    "inpuit_tensor.datatype = 'BYTES'\n",
    "#input_tensor.data_type = service_pb2.TYPE_FP32\n",
    "# input_tensor.data_type = service_pb2.DataType.TYPE_FP32\n",
    "input_tensor.contents.raw_contents = input_data.tobytes()\n",
    "\n",
    "# 출력 텐서 설정\n",
    "output_tensor = request.outputs.add()\n",
    "output_tensor.name = 'output_tensor'\n",
    "output_tensor.shape.extend([1, 1])\n",
    "output_tensor.data_type = service_pb2.TYPE_FP32\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.ModelInfer(request)\n",
    "print(\"추론 결과:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23a4f41a-63e7-4d60-b104-0e2e6f63f0fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"[request id: <id_unknown>] expected 1 inputs but got 0 inputs for model 'psng_predictor'\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-06-14T11:05:16.991053299+00:00\", grpc_status:3, grpc_message:\"[request id: <id_unknown>] expected 1 inputs but got 0 inputs for model \\'psng_predictor\\'\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 44\u001b[0m\n\u001b[0;32m     26\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     28\u001b[0m     \t{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     ]\n\u001b[0;32m     35\u001b[0m }\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 출력 텐서 설정\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# output_tensor = request.outputs['output_tensor']\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# output_tensor.dtype = \"FP32\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# output_tensor.shape.extend([1, 1])\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 추론 요청 전송\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelInfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m추론 결과:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m              request: Any,\n\u001b[0;32m   1023\u001b[0m              timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1027\u001b[0m              compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1028\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"[request id: <id_unknown>] expected 1 inputs but got 0 inputs for model 'psng_predictor'\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-06-14T11:05:16.991053299+00:00\", grpc_status:3, grpc_message:\"[request id: <id_unknown>] expected 1 inputs but got 0 inputs for model \\'psng_predictor\\'\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "# gRPC 서버 URL 설정\n",
    "url = 'localhost:8001'\n",
    "\n",
    "# gRPC 채널 생성\n",
    "channel = grpc.insecure_channel(url)\n",
    "stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# 추론 요청 설정\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = 'psng_predictor'\n",
    "request.model_version = \"1\"\n",
    "\n",
    "# 입력 텐서 설정\n",
    "# input_tensor = request.inputs['input_tensor']\n",
    "# input_tensor.dtype = \"FP32\"\n",
    "# input_tensor.shape.extend([1, 250])\n",
    "# input_data = np.random.rand(1, 250).astype(np.float32)\n",
    "# input_tensor.contents.raw_contents = input_data.tobytes()\n",
    "\n",
    "array = np.zeros([1,250]).tolist()\n",
    "payload = {\n",
    "\t\"inputs\": [\n",
    "    \t{\n",
    "            \"name\": \"input0\",\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"shape\": [1,250],\n",
    "            \"data\": array\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# 출력 텐서 설정\n",
    "# output_tensor = request.outputs['output_tensor']\n",
    "# output_tensor.dtype = \"FP32\"\n",
    "# output_tensor.shape.extend([1, 1])\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.ModelInfer(request)\n",
    "print(\"추론 결과:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe9e5e81-a909-4fba-90d6-2579f28ed2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b59ce-77c9-4a8d-85f7-5cdbf67ce240",
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(self, data: Image) -> List[float]:\n",
    "        preprocessed = self.preprocess_transformer.transform(data)\n",
    "        input_tensor = onnx_ml_pb2.TensorProto()\n",
    "        input_tensor.dims.extend(preprocessed.shape)\n",
    "        input_tensor.data_type = 1\n",
    "        input_tensor.raw_data = preprocessed.tobytes()\n",
    "        request_message = predict_pb2.PredictRequest()\n",
    "        request_message.inputs[self.onnx_input_name].data_type = input_tensor.data_type\n",
    "        request_message.inputs[self.onnx_input_name].dims.extend(preprocessed.shape)\n",
    "        request_message.inputs[self.onnx_input_name].raw_data = input_tensor.raw_data\n",
    "        response = self.stub.Predict(request_message)  # gRPC로 추론서버에 추론요청\n",
    "        output = np.frombuffer(response.outputs[self.onnx_output_name].raw_data, dtype=np.float32)\n",
    "        softmax = self.softmax_transformer.transform(output).tolist()\n",
    "        logger.info(f\"predict proba {softmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "492b9528-4316-4afd-8783-af35cc8eae13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"[request id: <id_unknown>] unexpected shape for input 'input_tensor' for model 'psng_predictor'. Expected [-1,1,250], got [1,250]\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-06-14T13:00:53.390085247+00:00\", grpc_status:3, grpc_message:\"[request id: <id_unknown>] unexpected shape for input \\'input_tensor\\' for model \\'psng_predictor\\'. Expected [-1,1,250], got [1,250]\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m output_tensor\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m request\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mextend([output_tensor])\n\u001b[1;32m---> 36\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgrpc_stub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelInfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 추론 요청 전송\u001b[39;00m\n\u001b[0;32m     41\u001b[0m response \u001b[38;5;241m=\u001b[39m stub\u001b[38;5;241m.\u001b[39mModelInfer(request)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m              request: Any,\n\u001b[0;32m   1023\u001b[0m              timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1027\u001b[0m              compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1028\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"[request id: <id_unknown>] unexpected shape for input 'input_tensor' for model 'psng_predictor'. Expected [-1,1,250], got [1,250]\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-06-14T13:00:53.390085247+00:00\", grpc_status:3, grpc_message:\"[request id: <id_unknown>] unexpected shape for input \\'input_tensor\\' for model \\'psng_predictor\\'. Expected [-1,1,250], got [1,250]\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "# gRPC 서버 URL 설정\n",
    "url = 'localhost:8001'\n",
    "\n",
    "# gRPC 채널 생성\n",
    "channel = grpc.insecure_channel(url)\n",
    "stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "input_data = np.random.randint(0, 10, size=(1, 250), dtype=np.int32).flatten().tolist()\n",
    "\n",
    "# 추론 요청 설정\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = 'psng_predictor'\n",
    "request.model_version = \"1\"\n",
    "\n",
    "# 입력 텐서 설정\n",
    "# input_tensor = request.inputs.add()  # 수정된 부분\n",
    "input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "input_tensor.name = \"input_tensor\"\n",
    "input_tensor.datatype = \"INT32\"\n",
    "input_tensor.shape.extend([1, 250])\n",
    "# input_tensor.contents.int_contents[:] = input_data\n",
    "input_tensor.contents.int_contents.extend(input_data)  # 수정된 부분\n",
    "\n",
    "request.inputs.extend([input_tensor])\n",
    "\n",
    "# 출력 텐서 설정\n",
    "output_tensor = service_pb2.ModelInferRequest().InferRequestedOutputTensor()\n",
    "output_tensor.name = \"output_tensor\"\n",
    "request.outputs.extend([output_tensor])\n",
    "\n",
    "response = grpc_stub.ModelInfer(request)\n",
    "\n",
    "\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.ModelInfer(request)\n",
    "#print(\"추론 결과:\\n\", response)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e17dae17-3431-474b-9d73-9a7be9d27970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = np.random.randint(0, 10, size=(1, 250), dtype=np.int32).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ccd329f-ec38-478f-9022-54c323515c08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a154f2d1-e760-423b-b260-affb771cc8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"unexpected explicit tensor data for input tensor 'input_tensor' for model 'psng_predictor' of type 'INT32', expected datatype '<invalid>'\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"unexpected explicit tensor data for input tensor \\'input_tensor\\' for model \\'psng_predictor\\' of type \\'INT32\\', expected datatype \\'<invalid>\\'\", grpc_status:3, created_time:\"2023-06-14T11:55:47.599098028+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 36\u001b[0m\n\u001b[0;32m     23\u001b[0m input_tensor\u001b[38;5;241m.\u001b[39mcontents\u001b[38;5;241m.\u001b[39mint_contents\u001b[38;5;241m.\u001b[39mextend(input_data\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 출력 텐서 설정\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# output_tensor = service_pb2.ModelInferRequest().InferRequestedOutputTensor()\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# output_tensor.name = \"output_tensor\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 추론 요청 전송\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelInfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m              request: Any,\n\u001b[0;32m   1023\u001b[0m              timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1027\u001b[0m              compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1028\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\grpc\\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"unexpected explicit tensor data for input tensor 'input_tensor' for model 'psng_predictor' of type 'INT32', expected datatype '<invalid>'\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"unexpected explicit tensor data for input tensor \\'input_tensor\\' for model \\'psng_predictor\\' of type \\'INT32\\', expected datatype \\'<invalid>\\'\", grpc_status:3, created_time:\"2023-06-14T11:55:47.599098028+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "# gRPC 서버 URL 설정\n",
    "url = 'localhost:8001'\n",
    "\n",
    "# gRPC 채널 생성\n",
    "channel = grpc.insecure_channel(url)\n",
    "stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# 추론 요청 설정\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = 'psng_predictor'\n",
    "request.model_version = \"1\"\n",
    "\n",
    "# 입력 텐서 설정\n",
    "input_tensor = request.inputs.add()\n",
    "input_tensor.name = 'input_tensor'\n",
    "input_tensor.shape.extend([1, 250])\n",
    "input_data = np.random.randint(0, 10, size=(1, 250), dtype=np.int32)\n",
    "input_tensor.contents.int_contents.extend(input_data.flatten().tolist())\n",
    "\n",
    "# 출력 텐서 설정\n",
    "# output_tensor = service_pb2.ModelInferRequest().InferRequestedOutputTensor()\n",
    "# output_tensor.name = \"output_tensor\"\n",
    "# response = grpc_stub.ModelInfer(request)\n",
    "\n",
    "#output_tensor = request.outputs.add()\n",
    "#output_tensor.name = 'output_tensor'\n",
    "#output_tensor.shape.extend([1, 1])\n",
    "#output_tensor.dims.extend([1, 1])\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.ModelInfer(request)\n",
    "#print(\"추론 결과:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8ba4c-7cc5-4185-9e32-869f4976d301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ced600fb-9fcb-4d31-b118-b229f7da1ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_tensor_proto() missing 1 required positional argument: 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m request\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mCopyFrom(input_tensor)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 출력 텐서 설정\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_tensor_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m request\u001b[38;5;241m.\u001b[39moutput_filter\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m request\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mCopyFrom(output_tensor)\n",
      "\u001b[1;31mTypeError\u001b[0m: make_tensor_proto() missing 1 required positional argument: 'values'"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:8001')\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "# 추론 요청 생성\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'psng_predictor'\n",
    "request.model_spec.signature_name = 'serving_default'\n",
    "\n",
    "# 입력 텐서 설정\n",
    "input_tensor = tf.make_tensor_proto(input_data, dtype=tf.float32, shape=[1, 250])\n",
    "request.inputs['input_tensor'].CopyFrom(input_tensor)\n",
    "\n",
    "# 출력 텐서 설정\n",
    "output_tensor = tf.make_tensor_proto(dtype=tf.float32, shape=[1, 1])\n",
    "request.output_filter.append('output_tensor')\n",
    "request.outputs['output_tensor'].CopyFrom(output_tensor)\n",
    "\n",
    "\n",
    "# 추론 요청 전송\n",
    "response = stub.Predict(request)\n",
    "print(\"추론 결과:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8790ee6d-4369-4f59-80ee-e227462eef3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-serving-api\n",
      "  Downloading tensorflow_serving_api-2.12.1-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: tensorflow<3,>=2.12.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-serving-api) (2.12.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-serving-api) (4.23.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-serving-api) (1.54.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.12.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.23.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (65.6.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (22.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.14.1)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.12.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.12.3)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.4.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (4.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (3.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.31.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (16.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.10.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.19.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (3.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pigiraffe0\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow<3,>=2.12.0->tensorflow-serving-api) (3.2.2)\n",
      "Installing collected packages: tensorflow-serving-api\n",
      "Successfully installed tensorflow-serving-api-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e3da8-dc68-4717-9170-cc50a951d898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792aa055-120a-4e96-9a14-5ce84dd59a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "569d7dcd-ffb8-4581-aac7-42be3377da2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-u URL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\PiGiraffe0\\AppData\\Roaming\\jupyter\\runtime\\kernel-8b5cdce0-8a35-49e6-9561-c317c36d18b6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions\n",
    "# are met:\n",
    "#  * Redistributions of source code must retain the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer.\n",
    "#  * Redistributions in binary form must reproduce the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer in the\n",
    "#    documentation and/or other materials provided with the distribution.\n",
    "#  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived\n",
    "#    from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
    "# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
    "# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
    "# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
    "# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
    "# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
    "# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
    "# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "import argparse\n",
    "\n",
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-v',\n",
    "                    '--verbose',\n",
    "                    action=\"store_true\",\n",
    "                    required=False,\n",
    "                    default=False,\n",
    "                    help='Enable verbose output')\n",
    "parser.add_argument('-u',\n",
    "                    '--url',\n",
    "                    type=str,\n",
    "                    required=False,\n",
    "                    default='localhost:8001',\n",
    "                    help='Inference server URL. Default is localhost:8001.')\n",
    "\n",
    "FLAGS = parser.parse_args()\n",
    "\n",
    "model_name = \"psng_predictor\"\n",
    "model_version = \"1\"\n",
    "batch_size = 1\n",
    "\n",
    "# Create gRPC stub for communicating with the server\n",
    "channel = grpc.insecure_channel(FLAGS.url)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "# Health\n",
    "try:\n",
    "    request = service_pb2.ServerLiveRequest()\n",
    "    response = grpc_stub.ServerLive(request)\n",
    "    print(\"server {}\".format(response))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "request = service_pb2.ServerReadyRequest()\n",
    "response = grpc_stub.ServerReady(request)\n",
    "print(\"server {}\".format(response))\n",
    "\n",
    "request = service_pb2.ModelReadyRequest(name=model_name,\n",
    "                                        version=model_version)\n",
    "response = grpc_stub.ModelReady(request)\n",
    "print(\"model {}\".format(response))\n",
    "\n",
    "# Metadata\n",
    "request = service_pb2.ServerMetadataRequest()\n",
    "response = grpc_stub.ServerMetadata(request)\n",
    "print(\"server metadata:\\n{}\".format(response))\n",
    "\n",
    "request = service_pb2.ModelMetadataRequest(name=model_name,\n",
    "                                           version=model_version)\n",
    "response = grpc_stub.ModelMetadata(request)\n",
    "print(\"model metadata:\\n{}\".format(response))\n",
    "\n",
    "# Configuration\n",
    "request = service_pb2.ModelConfigRequest(name=model_name,\n",
    "                                         version=model_version)\n",
    "response = grpc_stub.ModelConfig(request)\n",
    "print(\"model config:\\n{}\".format(response))\n",
    "\n",
    "# Infer\n",
    "request = service_pb2.ModelInferRequest()\n",
    "request.model_name = model_name\n",
    "request.model_version = model_version\n",
    "request.id = \"my request id\"\n",
    "\n",
    "input = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "input.name = \"input\"\n",
    "input.datatype = \"FP32\"\n",
    "input.shape.extend([1, 299, 299, 3])\n",
    "request.inputs.extend([input])\n",
    "\n",
    "output = service_pb2.ModelInferRequest().InferRequestedOutputTensor()\n",
    "output.name = \"InceptionV3/Predictions/Softmax\"\n",
    "request.outputs.extend([output])\n",
    "\n",
    "request.raw_input_contents.extend([bytes(1072812 * 'a', 'utf-8')])\n",
    "\n",
    "response = grpc_stub.ModelInfer(request)\n",
    "print(\"model infer:\\n{}\".format(response))\n",
    "\n",
    "print(\"PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda50cfe-8088-469f-8f42-12654e9433e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483fc04-9af9-4899-a827-074abcc2db1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74673e1-9457-4d1a-b967-936df9b83cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a6454-ed54-4b9f-bc44-c38612cb5d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a1257-8c91-40b2-ab94-f410b420b6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5886bc-527f-45ce-b6ce-5f537dfea3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b4d90-d659-4c1f-a108-4e0cd1880fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f4d14-819e-450b-8582-6871a7596750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c3f9c-fcdf-449f-8723-ed190fb2f076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa217442-b532-4d7b-b4fe-6c7112c6fe85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import grpc\n",
    "import numpy as np\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "\n",
    "class TritonInferenceClient:\n",
    "    def __init__(self, url):\n",
    "        self.channel = grpc.insecure_channel(url)\n",
    "        self.grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(self.channel)\n",
    "\n",
    "    # 이전 메서드 생략...\n",
    "\n",
    "    def infer(self, model_name, model_version=\"\"):\n",
    "        request = service_pb2.ModelInferRequest()\n",
    "        request.model_name = model_name\n",
    "        request.model_version = model_version\n",
    "        request.id = \"my request id\"\n",
    "\n",
    "        input_tensor = service_pb2.ModelInferRequest().InferInputTensor()\n",
    "        input_tensor.name = \"input\"\n",
    "        input_tensor.datatype = \"FP32\"\n",
    "        input_tensor.shape.extend([1, 250])  # 수정된 부분\n",
    "        request.inputs.extend([input_tensor])\n",
    "\n",
    "        # 입력 데이터 준비\n",
    "        input_data = np.random.rand(1, 250)  # 입력 데이터를 적절히 수정하세요\n",
    "        input_tensor_contents = input_data.tobytes()\n",
    "        request.raw_input_contents.extend([input_tensor_contents])\n",
    "\n",
    "        response = self.grpc_stub.ModelInfer(request)\n",
    "        print(\"model infer:\\n{}\".format(response))\n",
    "\n",
    "        print(\"PASS\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-v', '--verbose', action=\"store_true\", required=False, default=False, help='Enable verbose output')\n",
    "#     parser.add_argument('-u', '--url', type=str, required=False, default='localhost:8001', help='Inference server URL. Default is localhost:8001.')\n",
    "\n",
    "#     FLAGS = parser.parse_args()\n",
    "\n",
    "#     model_name = \"inception_graphdef\"\n",
    "#     model_version = \"\"\n",
    "\n",
    "#     client = TritonInferenceClient(FLAGS.url)\n",
    "#     client.check_server_live()\n",
    "#     client.check_server_ready()\n",
    "#     client.check_model_ready(model_name, model_version)\n",
    "#     client.get_server_metadata()\n",
    "#     client.get_model_metadata(model_name, model_version)\n",
    "#     client.get_model_config(model_name, model_version)\n",
    "#     client.infer(model_name, model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc215564-65c2-41ec-878e-1767e4c5d644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-u URL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\PiGiraffe0\\AppData\\Roaming\\jupyter\\runtime\\kernel-8b5cdce0-8a35-49e6-9561-c317c36d18b6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "FLAGS = parser.parse_args()\n",
    "\n",
    "model_name = \"psng_predicroe\"\n",
    "model_version = \"1\"\n",
    "\n",
    "client = TritonInferenceClient(FLAGS.url)\n",
    "client.check_server_live()\n",
    "client.check_server_ready()\n",
    "client.check_model_ready(model_name, model_version)\n",
    "client.get_server_metadata()\n",
    "client.get_model_metadata(model_name, model_version)\n",
    "client.get_model_config(model_name, model_version)\n",
    "client.infer(model_name, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc709c33-f1f0-471f-9a50-3ade2bc32f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
