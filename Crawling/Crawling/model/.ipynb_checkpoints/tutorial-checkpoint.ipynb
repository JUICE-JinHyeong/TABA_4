{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Sentiment Classification using GPT-2\n",
    "This is a tutorial using KR3. We classify the reviews in KR3 **without any training**. This is known as *zero-shot*. We use GPT-2, the generative model. Basically, we compare the probability of positive and negative token coming after the input review.\n",
    "\n",
    "Concepts\n",
    "- Zero-shot text classification\n",
    "- GPT-2\n",
    "\n",
    "Libraries\n",
    "- Datasets \n",
    "- Transformers\n",
    "- PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "We load the dataset from hugging face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:16:46.519130Z",
     "iopub.status.busy": "2022-03-26T03:16:46.518755Z",
     "iopub.status.idle": "2022-03-26T03:16:51.616245Z",
     "shell.execute_reply": "2022-03-26T03:16:51.615376Z",
     "shell.execute_reply.started": "2022-03-26T03:16:46.519096Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      4\u001b[0m kr3 \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWittgensteinian/KR3\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m kr3 \u001b[38;5;241m=\u001b[39m kr3\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "kr3 = load_dataset(\"Wittgensteinian/KR3\", split='train')\n",
    "\n",
    "kr3 = kr3.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:16:51.618179Z",
     "iopub.status.busy": "2022-03-26T03:16:51.617915Z",
     "iopub.status.idle": "2022-03-26T03:16:51.667997Z",
     "shell.execute_reply": "2022-03-26T03:16:51.667155Z",
     "shell.execute_reply.started": "2022-03-26T03:16:51.618143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review'],\n",
       "    num_rows: 641762\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rating': Value(dtype='int32', id=None),\n",
       " 'Review': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to use ambiguous reviews, i.e. reviews whose rating is 2. These reviews weren't intended to be classified in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c26ce91939496a8c95cfd84225ee21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_binary = kr3.filter(lambda x: x['Rating'] != 2)\n",
    "kr3_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we use GPT-2 for classification\n",
    "Here we show how we classify the reviews, with only single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model and the tokenizer. Here we use [GPT-2 trained on Korean corpus](https://github.com/SKT-AI/KoGPT2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:16:51.669562Z",
     "iopub.status.busy": "2022-03-26T03:16:51.669296Z",
     "iopub.status.idle": "2022-03-26T03:17:21.117857Z",
     "shell.execute_reply": "2022-03-26T03:17:21.117095Z",
     "shell.execute_reply.started": "2022-03-26T03:16:51.669528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer (GPT-2 trained on Korean corpus)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", pad_token='<pad>')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how the input text is tokenized. Entries in `input_ids` are the tokens. It's worth noting that `tokenized_reviews['input_ids']` is a 2D tensor, instead of 1D tensor. This is because the tokenizer returned the batch. Batch will be later used in inference using the entire dataset.\n",
    "\n",
    "> It's fine to ignore `attention_mask` in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:35:29.331513Z",
     "iopub.status.busy": "2022-03-26T03:35:29.330843Z",
     "iopub.status.idle": "2022-03-26T03:35:29.352590Z",
     "shell.execute_reply": "2022-03-26T03:35:29.351542Z",
     "shell.execute_reply.started": "2022-03-26T03:35:29.331453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다. 모든 직원분들께서 전부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구워주십니다. 가격 저렴한 편 아니지만 맛은 준수합니다. 등심덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 않았습니다. 하이라이트는 된장찌개. 진짜 굿입니다. 버터 간장밥, 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 남겨뒀습니다.\n",
      "{'input_ids': tensor([[44381, 26367,  6958, 10161,  8191, 21154, 10637,  9777,  9355, 13669,\n",
      "          9777,  7235, 11732, 15846, 11686, 43752,  9266,  9466, 20387, 10286,\n",
      "         11714,  9244, 12041, 33684, 13364,  7130, 16691,  9548, 18401,  7671,\n",
      "          7285, 23916, 17483,  9826, 12524,   739, 18221, 13673,  8236,  7888,\n",
      "          9061,  9065,  9446, 18622, 10114,  8614, 12109, 26089,  8236,  7895,\n",
      "         12521, 11562, 29932,  9266, 22804, 32837, 22033, 37194,  9030,  7894,\n",
      "          7216, 16912, 15464,  9958, 16693,  9073, 11434, 15126,  8149,  9566,\n",
      "          9181, 31231,  9719,  8721, 14591,  6889, 25446,  9265,  7530,   739,\n",
      "          7723,  7723,  9328, 10171, 16691,  9078,  9131, 51000,  9498,  8168,\n",
      "          8326,  6841,   389, 23971, 15669, 21154,  9848,  8539, 49375,  7605,\n",
      "           387, 10187,  7616,  8146,  9092,  7847,  9030, 13348,  9267, 11355,\n",
      "          7661,  7991,  9337, 24860, 18525,  7268, 16691]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "review = kr3_binary['Review'][idx]\n",
    "label = kr3_binary['Rating'][idx]\n",
    "tokenized_review = tokenizer(review, return_tensors='pt')\n",
    "print(review)\n",
    "print(tokenized_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed the model with the tokenized review, and the model gives an output. Remember that we're trying to see the model's prediction at the end of the input text. Therefore, we see the last prediction (hence indexing via -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:38:29.671299Z",
     "iopub.status.busy": "2022-03-26T03:38:29.670580Z",
     "iopub.status.idle": "2022-03-26T03:38:31.436922Z",
     "shell.execute_reply": "2022-03-26T03:38:31.436153Z",
     "shell.execute_reply.started": "2022-03-26T03:38:29.671251Z"
    }
   },
   "outputs": [],
   "source": [
    "y = model(**tokenized_review)\n",
    "prediction = y.logits[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor represents the unnormalized(before passing softmax) probabilities of each token coming out in the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:38:34.204940Z",
     "iopub.status.busy": "2022-03-26T03:38:34.204436Z",
     "iopub.status.idle": "2022-03-26T03:38:34.213841Z",
     "shell.execute_reply": "2022-03-26T03:38:34.213120Z",
     "shell.execute_reply.started": "2022-03-26T03:38:34.204902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.5961, -6.9506, -5.8478,  ..., -1.8356, -5.1506, -3.0455],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the prediction of GPT-2, or token with the highest probability. This is not our interest though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:38:36.060340Z",
     "iopub.status.busy": "2022-03-26T03:38:36.060074Z",
     "iopub.status.idle": "2022-03-26T03:38:36.066215Z",
     "shell.execute_reply": "2022-03-26T03:38:36.065543Z",
     "shell.execute_reply.started": "2022-03-26T03:38:36.060311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그리고'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pair of typical sentences expressing good/bad sentiment. We select a pair of tokens to represent *positive* and *negative* respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:17:25.735491Z",
     "iopub.status.busy": "2022-03-26T03:17:25.735241Z",
     "iopub.status.idle": "2022-03-26T03:17:25.741210Z",
     "shell.execute_reply": "2022-03-26T03:17:25.740234Z",
     "shell.execute_reply.started": "2022-03-26T03:17:25.735443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁최고', '입', '니', '다']\n",
      "['▁별로', '였', '습니', '다']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('최고입니다')) \n",
    "print(tokenizer.tokenize('별로였습니다')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:17:25.743032Z",
     "iopub.status.busy": "2022-03-26T03:17:25.742515Z",
     "iopub.status.idle": "2022-03-26T03:17:25.753028Z",
     "shell.execute_reply": "2022-03-26T03:17:25.752190Z",
     "shell.execute_reply.started": "2022-03-26T03:17:25.742995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10281]\n",
      "[15126]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('최고')) # it means 'Best'\n",
    "print(tokenizer.encode('별로')) # it means 'Not good'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the probability assigned to token 'Best' higher than those assigned to token 'Not good'?  \n",
    "If it is, we predict this input as *positive*. Otherwise, we predict this input as *negative*.\n",
    "Remember that label==1 is for positive reviews, and 0 is for negative reviews.  \n",
    "Therefore, code below shows whether our prediction is right. And we're right here!! Yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T03:38:45.444862Z",
     "iopub.status.busy": "2022-03-26T03:38:45.444581Z",
     "iopub.status.idle": "2022-03-26T03:38:45.450982Z",
     "shell.execute_reply": "2022-03-26T03:38:45.450310Z",
     "shell.execute_reply.started": "2022-03-26T03:38:45.444833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((prediction[10281] > prediction[15126]) == label).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on entire dataset using PyTorch\n",
    "Now we make a prediction on the entire dataset. Do not make a mistake that it will be a simple for loop. That will take forever!  \n",
    "We use **PyTorch** to exploit GPU for faster inference. The concept of **batch** comes along, differing from the case of single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We tokenize every review in the dataset. Some of the processes done during tokenization are:\n",
    "- *truncation* is when you truncate(=cut) the input text because it's too long(i.e. it exceeds the max_length).\n",
    "- *padding* is when you add extra tokens in the end of the input text to create a batch. We do not pad here. Instead, we set dynamic padding when we create PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456774c781a64819a2e09b90f787384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/460 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize\n",
    "def tokenize_func(x):\n",
    "    return tokenizer(x['Review'], max_length=256, truncation=True)\n",
    "\n",
    "kr3_tokenized = kr3_binary.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new features: `attention_mask` and `input_ids`. These are the parameters for the model(GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we do not need the feature `Review`, we remove it. Plus, we set the format of this dataset as 'torch', as we're going to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_tokenized = kr3_tokenized.remove_columns(['Review'])\n",
    "kr3_tokenized.set_format('torch')\n",
    "kr3_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 Inference using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make PyTorch dataloader. See the exmaple batch.  \n",
    "- `Rating[i]` represents the label (0 or 1) for (*i+1*)th review in the batch.\n",
    "- `attention_mask[i]` and `input_ids[i]` are the tokenized (*i+1*)th review in the batch.\n",
    "\n",
    "Dynamic Padding\n",
    "> We set dynamic padding using `DataCollatorWithPadding` from `transformers`. Dynamic padding pads to the longest sequence in the batch, instead of padding to certain fixed length. In the example below, we can deduce that the longest sequence in the batch had a length of 117."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rating': torch.Size([8]), 'input_ids': torch.Size([8, 117]), 'attention_mask': torch.Size([8, 117])}\n"
     ]
    }
   ],
   "source": [
    "# pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch_size = 8\n",
    "data_loader = DataLoader(kr3_tokenized, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "# example batch\n",
    "batch = next(iter(data_loader))\n",
    "print({k:v.size() for k,v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up GPU. If you don't have or haven't set up GPU, it would be hard to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('GPU not ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the sentiment of each review via inference of GPT-2. This loop will take some time.\n",
    "\n",
    "\n",
    "> `input_lens` represents the length of each input text. This is used to obtain the token predicted right after the input text. \n",
    "\n",
    "> If you run out of GPU memory, try to reduce `max_length` (in tokenization) or `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "confusion_matrix = [[0,0],[0,0]]\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    batch = {k:v.to(device) for k,v in batch.items()} # move the data to the GPU\n",
    "    y = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask']) # forward\n",
    "    input_lens = batch['attention_mask'].sum(axis=1) # length of inputs in the batch\n",
    "\n",
    "    for i in range(len(batch['Rating'])):\n",
    "        next_token_prediction = y.logits[i, input_lens[i]-1] # output of the model for single review\n",
    "\n",
    "        # prediction result\n",
    "        predicted_label = (next_token_prediction[10281] > next_token_prediction[15126]).item() \n",
    "        true_label = batch['Rating'][i].item()\n",
    "        confusion_matrix[true_label][predicted_label] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33148,  37762],\n",
       "       [ 76503, 311608]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "confusion_np_matrix = np.array(confusion_matrix)\n",
    "confusion_np_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of 75% is better than random guess without knowing the distribution (50%) but worse than random guess known the distribution (85%). Note that best accuracy with pretrain-finetuning was [96%](https://wandb.ai/wittgensteinian/Parameter-Efficient-Tuning?workspace=user-wittgensteinian).\n",
    "\n",
    "The model especially struggles with negative label. This trend is also shown in standard pretrain-finetuning approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7510680339243738\n",
      "Precision for positive: 0.8919140166585569\n",
      "Precision for negative: 0.30230458454551257\n",
      "Recall for positive: 0.8028837111032668\n",
      "Recall for negative: 0.46746580172049074\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', confusion_np_matrix.diagonal().sum() / confusion_np_matrix.sum())\n",
    "print('Precision for positive:', confusion_np_matrix[1,1] / confusion_np_matrix[:,1].sum())\n",
    "print('Precision for negative:', confusion_np_matrix[0,0] / confusion_np_matrix[:,0].sum())\n",
    "print('Recall for positive:', confusion_np_matrix[1,1] / confusion_np_matrix[1,:].sum())\n",
    "print('Recall for negative:', confusion_np_matrix[0,0] / confusion_np_matrix[0,:].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try random tokens as indicator for sentiment and see what happens. The result looks terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14003 시에\n",
      "18372 조금씩\n"
     ]
    }
   ],
   "source": [
    "n1 = np.random.randint(100, tokenizer.vocab_size)\n",
    "n2 = np.random.randint(100, tokenizer.vocab_size)\n",
    "print(n1, tokenizer.decode([n1]))\n",
    "print(n2, tokenizer.decode([n2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a27987044a14062844903c4b6f1ca3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_confusion_matrix = [[0,0],[0,0]]\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    batch = {k:v.to(device) for k,v in batch.items()} # move the data to the GPU\n",
    "    y = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask']) # forward\n",
    "    input_lens = batch['attention_mask'].sum(axis=1) # length of inputs in the batch\n",
    "\n",
    "    for i in range(len(batch['Rating'])):\n",
    "        next_token_prediction = y.logits[i, input_lens[i]-1] # output of the model for single review\n",
    "\n",
    "        # prediction result\n",
    "        predicted_label = (next_token_prediction[n1] > next_token_prediction[n2]).item() \n",
    "        true_label = batch['Rating'][i].item()\n",
    "        random_confusion_matrix[true_label][predicted_label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 58097,  12813],\n",
       "       [334177,  53934]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_confusion_np_matrix = np.array(random_confusion_matrix)\n",
    "random_confusion_np_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.244065086346812\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', random_confusion_np_matrix.diagonal().sum() / random_confusion_np_matrix.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
