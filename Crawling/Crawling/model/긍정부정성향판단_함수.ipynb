{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-Czck9Mhbo9s"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAExXEXJkaw9",
        "outputId": "7866e6bb-ed1a-4e51-fc44-e4ce0e967d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "my_path = '/content/notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F0pftX3kiyf",
        "outputId": "f8c989d4-8acd-4a23-f8e1-ff37e0077546"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "# import matplotlib.pyplot as plt\n",
        "# import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "lZC2oZWykoS3"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "review_data_frm_04 = pd.read_excel('/content/drive/MyDrive/Crawling/제주특별자치도/제주시/식당정보/제주특별자치도_제주시_연동_리뷰전체모음_04.xlsx')"
      ],
      "metadata": {
        "id": "PbxKl2qA9ecG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class psng_predict_once() :\n",
        "  def __init__(self , df) :\n",
        "    self.df = df\n",
        "    \n",
        "    try :\n",
        "      self.review = df.리뷰\n",
        "    except :\n",
        "      try :\n",
        "        self.review = df.REVIEW\n",
        "      except :\n",
        "        print('DataFrame에 리뷰 or Review 칼럼이 존재하지 않거나, DataFrame 형식이 아닙니다.')\n",
        "        return\n",
        "    \n",
        "    # 경로 확인\n",
        "      # 모델 로드\n",
        "    self.loaded_model = load_model(r'/content/drive/MyDrive/data_review/archive/new_model_01_v2.h5')\n",
        "      #  텍스트 데이터를 토큰화하고 단어 사전을 구축하기위한 학습시 사용한 train_data 로드\n",
        "    train_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/train_data_main.csv' , encoding = 'euc-kr')\n",
        "      # 모델 성능 확인용 test_data\n",
        "    test_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/test_data_main.csv' , encoding = 'euc-kr')\n",
        "\n",
        "    # 데이터 정규화\n",
        "      # array(\"['a','b','c']\") -> array(list['a','b','c'])\n",
        "    X_train = train_data['tokenized'].apply(eval).values\n",
        "    self.X_test = test_data['tokenized'].apply(eval).values\n",
        "    self.y_test = np.array(test_data['label'].values)\n",
        "    \n",
        "    self.X_test_tkd = None\n",
        "\n",
        "    # tokenized option\n",
        "    vocab_size = 87912\n",
        "      # OOV \n",
        "    self.tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "    self.tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    self.max_len = 250\n",
        "\n",
        "  def preprocessing(self , R_frm) :\n",
        "    \n",
        "    stopwords = [ '도', '는', '다', '의', '가', '이', '은', \n",
        "                  '한', '에', '하', '고', '을', '를', '인', '듯', \n",
        "                  '과', '와', '네', '들', '듯', '지', '임', '게', \n",
        "                  '는', '이', '했', '슴', '음', '것', '거', '로',\n",
        "                  '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
        "                  '수', '할', '그', '함', '돈', '번', '두' ,'개',\n",
        "                  '건', '내', '저', '만', '갈', '걸', '제', '명', \n",
        "                  '분', \n",
        "                  '인데', '이가', '했었','해서',\n",
        "                  '습니다', '했는데', '입니다' ]\n",
        "\n",
        "    # 정규 표현식 수행\n",
        "    R_frm['리뷰'] = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', R_frm['리뷰'])\n",
        "    \n",
        "    # 공백은 Null 값으로 변경\n",
        "    if R_frm['리뷰'] == '' :\n",
        "      R_frm['리뷰'] = None\n",
        "      self.R_frm_tokenized_value = None\n",
        "      return self.R_frm_tokenized_value\n",
        "    else :\n",
        "      # konlpy okt -> 문장 토큰화 (모델 개선시 : konlpy 동사 형용사 명사 사전 업데이트)\n",
        "      encoded = okt.pos(R_frm['리뷰'])\n",
        "      # print(encoded)\n",
        "      encoded = [word for word,shape in encoded if shape in ['Verb' , 'Adjective' , 'Noun' , 'VerbPrefix'] if word not in stopwords]\n",
        "      self.R_frm_tokenized_value = encoded\n",
        "      \n",
        "      return self.R_frm_tokenized_value\n",
        "\n",
        "  def model_test(self) :\n",
        "    tokenizer = self.tokenizer\n",
        "    \n",
        "    X_test = tokenizer.texts_to_sequences(self.X_test)\n",
        "    y_test = self.y_test\n",
        "    X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
        "\n",
        "    print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
        "\n",
        "  def word_index(self) :\n",
        "    tokenizer = self.tokenizer\n",
        "    \n",
        "    print(tokenizer.word_index)\n",
        "\n",
        "  def predict(self) :\n",
        "\n",
        "    # 리뷰 하나씩 예측\n",
        "\n",
        "    R_frm = self.df.copy()\n",
        "    reviews = self.review.tolist()\n",
        "    max_len = self.max_len\n",
        "    tokenizer = self.tokenizer\n",
        "    result = []\n",
        "\n",
        "    for num , review in enumerate(reviews) :\n",
        "      \n",
        "      R_pred = self.preprocessing(R_frm.copy().iloc[num])\n",
        "      \n",
        "      if R_pred == None :\n",
        "        result.append('-1')\n",
        "      else :\n",
        "        pred = tokenizer.texts_to_sequences([R_pred])\n",
        "        pred = pad_sequences(pred , maxlen=max_len)\n",
        "        # print(pred)\n",
        "        score = float(self.loaded_model.predict(pred))\n",
        "\n",
        "        print(num)\n",
        "        print(review)\n",
        "        if score > 0.9 :\n",
        "          result.append('0')\n",
        "        elif (score < 0.1) :\n",
        "          result.append('1')\n",
        "        else :\n",
        "          result.append('2')\n",
        "        print()\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "aMMr73upqTHq"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = psng_predict_once(review_data_frm_04[:5])"
      ],
      "metadata": {
        "id": "wCEGK3c6l4eP"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.predict()"
      ],
      "metadata": {
        "id": "vx7b1p_Lm9Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "-Czck9Mhbo9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()"
      ],
      "metadata": {
        "id": "FDP96G8Ru4Yt"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model(r'/content/drive/MyDrive/data_review/archive/new_model_01_v2.h5')"
      ],
      "metadata": {
        "id": "K33KvBQvwVel"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/train_data_main.csv' , encoding = 'euc-kr')"
      ],
      "metadata": {
        "id": "qxTAE95hs_7-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/test_data_main.csv' , encoding = 'euc-kr')"
      ],
      "metadata": {
        "id": "R-ENmExgbpFI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array(\"['a','b','c']\") -> array(list['a','b','c'])\n",
        "X_train = train_data['tokenized'].apply(eval).values"
      ],
      "metadata": {
        "id": "fcoa5VAwvN6L"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj3Tu3jXvXDL",
        "outputId": "dfae316a-a3a2-4cd8-fc8e-b4be0f2379c3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['난', '별로', '달다', '먼저', '왔는데도', '늦게', '온', '테이블', '세팅', '나가고', '블로그', '맛', '집', '떠서', '가봤는데', '개인', '차', '큰', '같음']),\n",
              "       list(['짜가', '정말', '정말', '맛있었어요', '강', '추합니다', '또', '가고', '싶을', '정도']),\n",
              "       list(['어어', '어어어어', '무', '기대해서', '런가', '별로', '였다', '딱히', '특별하지', '않은', '맛', '결정', '패티', '그랬다']),\n",
              "       ..., list(['빵', '겁나', '맛없음', '커피']),\n",
              "       list(['음료', '미지', '밍밍', '증편', '떡', '구이', '따뜻하긴', '내어주시는', '발사믹', '강하고', '떡', '안', '어울려요', '오픈', '기간', '방문', '지금', '좀', '나아졋길', '바랍니다']),\n",
              "       list(['고등학생', '때', '진짜', '맛있게', '먹었던', '같은데', '사', '년', '지난', '후', '지금', '먹어', '보니', '달다', '악덕', '사서', '옆', '편의점', '테이블', '맥주', '사서', '먹으면', '꿀이긴'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_data['tokenized'].apply(eval).values"
      ],
      "metadata": {
        "id": "admtFeKm0MnB"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(test_data['label'].values)"
      ],
      "metadata": {
        "id": "etL_WfGX0Q3-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prpSeppn0Plr",
        "outputId": "448a78ba-ce66-4669-ecc6-3715000e647c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['맛', '괜찮은지', '모르는데', '계산', '하려면', '짜증', '캐셔', '좀', '늘려서', '해소', '해주면', '좋을', '성질', '급한', '사람', '그냥', '안', '사', '먹고', '말']),\n",
              "       list(['주위', '유명한', '집', '닫아서', '갔는데', '영업', '하시냐고', '주방', '할머니', '여쭤', '보니', '그냥', '들어오면', '되지', '물어봐라고', '미', '간', '찌푸리고', '짜증', '내며', '소리', '지르네요', '분노조절', '장애인', '맛', '짬뽕', '외', '성비', '최악', '결론', '불친절', '성비', '최악']),\n",
              "       list(['갑작스럽게', '급', '방문', '기대', '일도', '안', '하고', '가서', '런가', '맛있게', '잘', '먹었다', '여긴', '스텔라', '피자', '꼭', '먹어야', '된다']),\n",
              "       ...,\n",
              "       list(['바다', '가까운데', '뷰', '좋은', '아니고', '안', '좋은', '아니고', '애매함', '자리', '쟁탈전', '치열하고', '음료', '양', '퀄리티', '비해', '매우', '비싼', '편']),\n",
              "       list(['베트남', '맥주', '거리', '같은', '떠들썩하고', '와글와글', '젊은', '분위기']),\n",
              "       list(['사악한', '가격', '대체재', '없으므로', '찾게', '되는', '가게', '맛', '변하지', '않았으면'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzaYMkh81rTV",
        "outputId": "66ce3f27-ecc2-416c-d615-37e8d57ed4f4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_frm_04.리뷰.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Xp72mKtuMn",
        "outputId": "34290795-56d3-4cb9-b397-d315f4134ba0"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 17509 entries, 0 to 17508\n",
            "Series name: 리뷰\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "17509 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 136.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', R_frm.iloc[0].리뷰)\n",
        "new_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AouPZZNMwgfW",
        "outputId": "97da60c9-0f9c-4131-d354-3829a2f90cf8"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'좋아요'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_frm_04 = pd.read_excel('/content/drive/MyDrive/Crawling/제주특별자치도/제주시/식당정보/제주특별자치도_제주시_연동_리뷰전체모음_04.xlsx')"
      ],
      "metadata": {
        "id": "bxWMK9cG2zdc"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_frm = review_data_frm_04.copy()"
      ],
      "metadata": {
        "id": "ZYTjaV0u3ieY"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_frm.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSW5bhJg3xDq",
        "outputId": "e5824900-bd36-4513-c429-0b8a46fa3a00"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17509 entries, 0 to 17508\n",
            "Data columns (total 12 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   리뷰         17509 non-null  object\n",
            " 1   리뷰이미지url   6386 non-null   object\n",
            " 2   작성자프로필     17509 non-null  object\n",
            " 3   My플레이스     17509 non-null  object\n",
            " 4   작성자이름      17509 non-null  object\n",
            " 5   리뷰작성수      17509 non-null  object\n",
            " 6   리뷰작성일      17509 non-null  object\n",
            " 7   식당방문수      17509 non-null  object\n",
            " 8   리뷰인증방식     17509 non-null  object\n",
            " 9   식당id       17509 non-null  int64 \n",
            " 10  사업장명       17509 non-null  object\n",
            " 11  tokenized  17509 non-null  object\n",
            "dtypes: int64(1), object(11)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R_frm['리뷰'] = R_frm.리뷰.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
        "R_frm['리뷰'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', \n",
        "            '한', '에', '하', '고', '을', '를', '인', '듯', \n",
        "            '과', '와', '네', '들', '듯', '지', '임', '게' , \n",
        "            '는' ,'이','인데','이가','했었','했','습니다','슴','음' ,\n",
        "            '것' ,'거' ,'로' ,'들' ,'거' ,'것' ,'곳' , '분' , '원' ,\n",
        "            '입니다' ,'해서' ,'더' ,'왜' ,'수' ,'할' , '그' ,'함' ,'돈' ,'번' ,\n",
        "            '두' ,'개' ,'했는데' ,'건' ,'내' , '저' ,'만' ,'갈','걸','제','명' ,\n",
        "            '분' ,'해']\n",
        "R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
        "R_frm['tokenized'] = R_frm['tokenized'].apply(lambda x: [word for word,shape in x if shape in ['Verb' , 'Adjective' , 'Noun' , 'VerbPrefix'] if word not in stopwords])"
      ],
      "metadata": {
        "id": "MlZEm4AJ3eiE"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_pred = R_frm['tokenized'].values"
      ],
      "metadata": {
        "id": "LGS11XH_20IE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 문장씩 값을 반환해야하기에 배열을 쪼개주자\n",
        "t = R_frm['tokenized'].values"
      ],
      "metadata": {
        "id": "Fv7BWcxnTIJ7"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in t[:3] :\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gENvnW3Hett5",
        "outputId": "9c053c88-136b-4ec2-99e8-e4dd222d3af7"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['좋아요']\n",
            "['피낫', '버터', '쿠키앤크림', '맛있어요']\n",
            "['좋아요']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxVLVvcP4_Wz",
        "outputId": "21ba32a9-c086-4be3-8d64-2e73b556fa9b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['좋아요']), list(['피낫', '버터', '쿠키앤크림', '맛있어요']), list(['좋아요']),\n",
              "       ..., list(['굿']), list(['대화', '하기', '좋아요']), list(['굿'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size = 46827\n",
        "vocab_size = 87912\n",
        "# OOV\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "# X_train - 훈련 때 사용했던 data 필요\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# X_train - 예측시, 예측 데이터 필요\n",
        "# X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "R_pred = tokenizer.texts_to_sequences(R_pred)"
      ],
      "metadata": {
        "id": "cJ4l41fqsqJF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "EapGcwSj8yaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_pred[:3]"
      ],
      "metadata": {
        "id": "16a70oxf5Rcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[:3]"
      ],
      "metadata": {
        "id": "Q9WR3DXh06N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 250"
      ],
      "metadata": {
        "id": "Oy4HzYKT2gSX"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "R_pred = pad_sequences(R_pred , maxlen=max_len)"
      ],
      "metadata": {
        "id": "x20M_khB2bvE"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSiC8iRf2iWU",
        "outputId": "6972dc36-1807-4786-efd1-5293f47b605a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34966, 250)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcko3JQB5eII",
        "outputId": "cc740190-a44f-4ba6-c7c4-2b92d4f2086c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17509, 250)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(R_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJYHW5rY9qdD",
        "outputId": "4227d138-8993-4588-fb27-cd8fba8ecdbc"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0 ...     0     0    57]\n",
            " [    0     0     0 ...   605 36817    29]\n",
            " [    0     0     0 ...     0     0    57]\n",
            " ...\n",
            " [    0     0     0 ...     0     0   801]\n",
            " [    0     0     0 ...  1065   194    57]\n",
            " [    0     0     0 ...     0     0   801]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvUxH0KRb2cR",
        "outputId": "3e4d4a97-f32d-435c-fdc1-c7b26217aaf0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1093/1093 [==============================] - 86s 78ms/step - loss: 0.1658 - acc: 0.9370\n",
            "\n",
            " 테스트 정확도: 0.9370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.predict(R_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFcpl_DDcuKa",
        "outputId": "89995d04-3d34-4c0d-8fde-cf552498e81e"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "548/548 [==============================] - 41s 75ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92867976],\n",
              "       [0.7815909 ],\n",
              "       [0.92867976],\n",
              "       ...,\n",
              "       [0.8745687 ],\n",
              "       [0.6947712 ],\n",
              "       [0.8745687 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for num , review in enumerate(review_data_frm_04.리뷰.tolist()[:5]) :\n",
        "  print(num)\n",
        "  print(review)\n",
        "  print(float(loaded_model.predict(R_pred[num])))\n",
        "  print()"
      ],
      "metadata": {
        "id": "Wfj82CJPcuQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전체 돌리기"
      ],
      "metadata": {
        "id": "ToptzKTV5D9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class psng_predict_all() :\n",
        "  def __init__(self , df) :\n",
        "    self.df = df\n",
        "    \n",
        "    try :\n",
        "      self.review = df.리뷰\n",
        "    except :\n",
        "      try :\n",
        "        self.review = df.REVIEW\n",
        "      except :\n",
        "        print('DataFrame에 리뷰 or Review 칼럼이 존재하지 않거나, DataFrame 형식이 아닙니다.')\n",
        "        return\n",
        "    \n",
        "    # 경로 확인\n",
        "      # 모델 로드\n",
        "    self.loaded_model = load_model(r'/content/drive/MyDrive/data_review/archive/new_model_01_v2.h5')\n",
        "      #  텍스트 데이터를 토큰화하고 단어 사전을 구축하기위한 학습시 사용한 train_data 로드\n",
        "    train_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/train_data_main.csv' , encoding = 'euc-kr')\n",
        "      # 모델 성능 확인용 test_data\n",
        "    test_data = pd.read_csv(r'/content/drive/MyDrive/data_review/archive/test_data_main.csv' , encoding = 'euc-kr')\n",
        "\n",
        "    # 데이터 정규화\n",
        "      # array(\"['a','b','c']\") -> array(list['a','b','c'])\n",
        "    X_train = train_data['tokenized'].apply(eval).values\n",
        "    self.X_test = test_data['tokenized'].apply(eval).values\n",
        "    self.y_test = np.array(test_data['label'].values)\n",
        "    \n",
        "    self.X_test_tkd = None\n",
        "\n",
        "    # tokenized option\n",
        "    vocab_size = 87912\n",
        "      # OOV \n",
        "    self.tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "    self.tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    self.max_len = 250\n",
        "\n",
        "  def preprocessing(self) :\n",
        "    R_frm = self.df.copy()\n",
        "    \n",
        "    # 정규 표현식 수행\n",
        "    R_frm['리뷰'] = R_frm.리뷰.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) \n",
        "    # 공백은 Null 값으로 변경\n",
        "    R_frm['리뷰'].replace('', np.nan, inplace=True) \n",
        "    R_frm['리뷰'] = R_frm['리뷰'].astype(str)\n",
        "    stopwords = ['도', '는', '다', '의', '가', '이', '은', \n",
        "                 '한', '에', '하', '고', '을', '를', '인', '듯', \n",
        "                 '과', '와', '네', '들', '듯', '지', '임', '게', \n",
        "                 '는', '이', '했', '슴', '음', '것', '거', '로',\n",
        "                 '들', '거', '곳', '분', '원', '더', '왜', '해',\n",
        "                 '수', '할', '그', '함', '돈', '번', '두' ,'개',\n",
        "                 '건', '내', '저', '만', '갈', '걸', '제', '명', \n",
        "                 '분', \n",
        "                 '인데', '이가', '했었','해서',\n",
        "                 '습니다', '했는데', '입니다']\n",
        "\n",
        "    # konlpy okt -> 문장 토큰화 (모델 개선시 : konlpy 동사 형용사 명사 사전 업데이트)\n",
        "    R_frm['tokenized'] = R_frm['리뷰'].apply(okt.pos)\n",
        "    R_frm['tokenized'] = R_frm['tokenized'].apply(lambda x: [word for word,shape in x if shape in ['Verb' , 'Adjective' , 'Noun' , 'VerbPrefix'] if word not in stopwords])\n",
        "    R_pred = R_frm['tokenized'].values\n",
        "    return R_pred\n",
        "\n",
        "  def model_test(self) :\n",
        "    tokenizer = self.tokenizer\n",
        "    \n",
        "    X_test = tokenizer.texts_to_sequences(self.X_test)\n",
        "    y_test = self.y_test\n",
        "    X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
        "\n",
        "    print(\"\\n 테스트 정확도: %.4f\" % (self.loaded_model.evaluate(X_test, y_test)[1]))\n",
        "\n",
        "  def word_index(self) :\n",
        "    tokenizer = self.tokenizer\n",
        "    \n",
        "    print(tokenizer.word_index)\n",
        "\n",
        "  def predict(self) :\n",
        "    R_pred = self.preprocessing()\n",
        "    max_len = self.max_len\n",
        "    tokenizer = self.tokenizer\n",
        "    result = []\n",
        "    pred = tokenizer.texts_to_sequences(R_pred)\n",
        "    pred = pad_sequences(pred , maxlen=max_len)\n",
        "    print(np.array(pred))\n",
        "    score = self.loaded_model.predict(pred)\n",
        "    return score\n",
        "  \n",
        "  def prediction(self) :\n",
        "    result = []\n",
        "    score = self.predict()\n",
        "    \n",
        "    for num in range(len(score)) :\n",
        "      if score[num][0] > 0.9 :\n",
        "        result.append('0')\n",
        "      elif score[num][0] < 0.1 :\n",
        "        result.append('1')\n",
        "      else :\n",
        "        result.append('2')\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "    \n",
        "  \n"
      ],
      "metadata": {
        "id": "dTig-Hrf-VEZ"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oo = psng_predict_all(review_data_frm_04[:5])"
      ],
      "metadata": {
        "id": "O5uIDr6Q9sSR"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oo.prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi0vPDTe9zqZ",
        "outputId": "fcfb69bd-a709-4b59-bc4b-5bd4a7b1dc47"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0 ...     0     0    57]\n",
            " [    0     0     0 ...   605 36817    29]\n",
            " [    0     0     0 ...     0     0    57]\n",
            " [    0     0     0 ...     0     0   801]\n",
            " [    0     0     0 ...     0     0   801]]\n",
            "1/1 [==============================] - 1s 812ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', '2', '0', '2', '2']"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcYhMqXE-4fa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}